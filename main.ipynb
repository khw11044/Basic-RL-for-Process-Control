{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요한 라이브러리들을 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 예제 데이터를 생성한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 품질 (Property, y)을 Setting한 Point에 맞추기 위해 온도 (Temperature, x)를 어떻게 운전해야하나?를 취적화 하는 것을 목적으로 하는 상황이며, x와 y는 아주 강한 선형성을 띈다고 가정한다.\n",
    "\n",
    "아래 생성한 데이터는 \"그동안 temperature (x)를 이런식으로 운전했을 때, 제품의 품질 (y)는 이랬다\"라는 데이터라고 이해하면 된다.\n",
    "\n",
    "아래 그래프로 시각화 하였을 때, 온도(x)가 높을 수록 품질(y)는 상승한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 데이터 생성 \n",
    "import pandas as pd \n",
    "temp = [random.randint(-10,100) for _ in range(100)]\n",
    "property = [i*3.5 + 10 + random.random()*2 for i in temp]\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'x': temp,\n",
    "    'y':property\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89</td>\n",
       "      <td>323.432665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67</td>\n",
       "      <td>245.620741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68</td>\n",
       "      <td>248.491711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>132.501766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>39.427172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x           y\n",
       "0  89  323.432665\n",
       "1  67  245.620741\n",
       "2  68  248.491711\n",
       "3  35  132.501766\n",
       "4   8   39.427172"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='x', ylabel='y'>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.plot(x='x', y='y', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "episode를 스스로 생성하기 위한 state, reward, action 등 environment를 정의한다. \n",
    "\n",
    "상황을 개발자가 원하는대로 정의하면 되지만, 공정의 경우 모든 episode 상황에 대한 결과가 어떨지 실제로 실험해보기에는 현실적으로 불가능에 가깝다.\n",
    "\n",
    "따라서, 관심있는 공정에서 x값이 ~할때, y값이 어떨지를 추정할 수 있는 x와 y의 Model이 필요하다.\n",
    "\n",
    "애초에 선형성이 강한 가상의 환경을 정의했으므로 Linear Regression 모델을 생성한다. \n",
    "\n",
    "(당연히 x와 y의 Linear Regression 모델을 알고 있다면 y값을 어떤 setting point로 만들기 위해 x를 어떤 값으로 해야할지는 굳이 강화학습과 같은 최적화 알고리즘을 하지 않아도 쉽게 계산이 가능하다. 그러나, 지금은 최대한 단순한 가상의 환경을 가정하여 진행한다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.49914751]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression().fit(np.array(temp).reshape(-1,1), np.array(property).reshape(-1,1))\n",
    "\n",
    "print(model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model의 coef를 확인해보면 3.5에 근접한다. 애초에 x*3.5로 y를 만들었으니 그럴 수 밖에 없다. 학습이 아주 잘되었다.\n",
    "\n",
    "이제 위에서 생성한 모델을 기반으로 Environment Class를 정의한다. \n",
    "\n",
    "Environment Class에는 생성자 외에 세 개 function이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, z, setPoint):\n",
    "        self.z = z \n",
    "        self.setPoint = setPoint \n",
    "        self.terminated = False \n",
    "        self.state = None \n",
    "        \n",
    "    def reset(self):\n",
    "        yPred = model.predict(np.array([self.z]).reshape(-1,1)).item()\n",
    "        self.state = torch.tensor([self.setPoint - yPred])\n",
    "        #self.state = torch.tensor(self.state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        return self.state \n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            self.z -= 1\n",
    "        elif action == 1:\n",
    "            self.z += 1\n",
    "        \n",
    "        if self.z < -10 or self.z > 100:\n",
    "            reward = -100. \n",
    "            self.terminated = True \n",
    "            return None, torch.tensor([reward]), self.terminated \n",
    "        \n",
    "        yPred = model.predict(np.array([self.z]).reshape(-1,1)).item()\n",
    "        self.state = torch.tensor([self.setPoint - yPred])\n",
    "        #self.state = torch.tensor(self.state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        if abs(self.state) <= 5:\n",
    "            self.terminated = True \n",
    "        reward = 1 / abs(self.state) * 5\n",
    "        return self.state, reward, self.terminated \n",
    "    \n",
    "    def render(self):\n",
    "        return self.z "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 생성자를 보면 z와 setPoint를 초기값으로 받는다, z는 현재 x값을 의미하고 setPoint는 y값을 어떤 값으로 Setting하고 싶은지에 대한 값을 의미한다. \n",
    "\n",
    "\n",
    "**reset()** 은 매 episode마다 맨 처음 state를 reset하는 역할을 수행한다. 가능한 범위 내 random한 가상의 state 값을 return한다.\n",
    "\n",
    "state는 (setPoint - y)값으로 한다.\n",
    "\n",
    "첫, 이 state를 최소화하는 것이 목적이다.\n",
    " \n",
    "**step()** 은 action을 input으로 받아 현 state에서 action을 취한 다음 'state, reward, 종료인지 아니지'에 대한 정보를 return한다.\n",
    "\n",
    "본 과정에서 action은 0과 1로 정의한다.\n",
    "action이 0이면 x를 1만큼 낮추고, action이 1이면 x를 1만큼 키운다.\n",
    "\n",
    "reward는 1/abs(setPoint - y)로 정의한다.\n",
    "setting한 값과 현재 y값과의 차이가 작을수록 큰 reward를 발리한다.\n",
    "\n",
    "**y는 위에서 생성한 모델에 x를 넣어 prediction하여 계산한다. 이 과정을 위해 x와 y의 model을 만들었다고 이해하면 된다.**\n",
    "\n",
    "abs(setPoint - y)값이 5보다 작으면 최적화 되었다고 판단하여 episode를 종료한다.\n",
    "\n",
    "또한, action을 취한 후 x가 정의한 min ~ max range를 벗어나면 -100의 reward로 return하며 episode는 종료된다.\n",
    "\n",
    "**render()** 는 episode가 어떻게 진행되고 있는지 과정을 확인하기 위한 function으로, 꼭 필요한 function은 아니며 원하는 부분을 자유롭게 구현하면 된다. 여기서는 현재 x (z) 값을 return하도록 구현하였다.\n",
    "\n",
    "이렇듯 'state, action, reward, episode terminate 조건' 등 모든 환경의 상황과 최대한 유사하게, 효율적으로 모델이 최적화 할 수 있도록 정의되어야 하며 이 과정에는 Biz.domain이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reinforcement Model 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정의한 가상환경이 굉장히 단순하므로 Reinforcement Model도 아주 단순하게 구현한다.\n",
    "\n",
    "현 state를 입력받아 최적의 action을 return하는 역할을 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, 16, bias=True)\n",
    "        self.linear2 = nn.Linear(16, outputs, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return torch.unsqueeze(F.log_softmax(x, dim=0), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Select Action 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "바로 위에서 정의한 reinforcement model에 기반하여 현 state를 입력으로 받아 다음 action을 return하는 function을 정의한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_actions = 2  # action 갯수\n",
    "policy_net = DQN(n_actions).to(device)\n",
    "target_net = DQN(n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "def select_action(state):\n",
    "    global steps_done \n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1]\n",
    "    else:\n",
    "        return torch.tensor([random.randrange(n_actions)], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policy network에 의해서만 action을 취하다보면 local minimum에 빠질 수 있으므로 가끔 random한 선택을 하게 해준다.\n",
    "\n",
    "eps_threshold가 random한 어떤 값보다 작으면 policy network가 return한 action을 return하고, random한 어떤 값보다 크면 random한 action을 return한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "def plot_durations(is_ipython=is_ipython,show_result=False):\n",
    "    plt.figure(1)\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    if is_ipython:\n",
    "        plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Traning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 위에서 만든 모든 것들을 조합하여 학습을 진행한다.\n",
    "\n",
    "먼저 학습할 episode들을 저장할 memory를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"transition 저장\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimize function도 정의한다.\n",
    "\n",
    "이 부분은 강화학습 Q-Learning을 이해하고 있어야 이해할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return \n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.stack(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    state_action_values = policy_net(state_batch).squeeze().gather(1, action_batch.unsqueeze(1))\n",
    "    \n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states.reshape(non_final_next_states.size()[0],1)).squeeze().max(1)[0].detach()\n",
    "    \n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1,1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Action(x):\n",
    "    if x.item() == 0:\n",
    "        return \"Down\"\n",
    "    else:\n",
    "        return \"Up\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 실제 episode를 진행하면서 강화학습 모델을 학습한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setPoint (특정 y값): 290 을 맞추기위해 위해\n",
      "Episode: 0/101 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.1679338812828064\n",
      "실패! 1 / 1\n",
      "\n",
      "setPoint (특정 y값): 99 을 맞추기위해 위해\n",
      "Episode: 1/29 - Action 이후 x 값: 26 / 수행 action: Down / 다음상태 y 값: tensor([-2.9013]) / reward: 1.72\n",
      "loss: 0.12434439361095428\n",
      "성공! 1 / 2\n",
      "\n",
      "setPoint (특정 y값): 102 을 맞추기위해 위해\n",
      "Episode: 2/40 - Action 이후 x 값: 27 / 수행 action: Down / 다음상태 y 값: tensor([-3.4004]) / reward: 1.47\n",
      "loss: 0.2566681206226349\n",
      "성공! 2 / 3\n",
      "\n",
      "setPoint (특정 y값): 321 을 맞추기위해 위해\n",
      "Episode: 3/72 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.2047833502292633\n",
      "실패! 2 / 4\n",
      "\n",
      "setPoint (특정 y값): -22 을 맞추기위해 위해\n",
      "Episode: 4/7 - Action 이후 x 값: -8 / 수행 action: Down / 다음상태 y 값: tensor([-4.9303]) / reward: 1.01\n",
      "loss: 0.22124965488910675\n",
      "성공! 3 / 5\n",
      "\n",
      "setPoint (특정 y값): 258 을 맞추기위해 위해\n",
      "Episode: 5/34 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.258463978767395\n",
      "실패! 3 / 6\n",
      "\n",
      "setPoint (특정 y값): 44 을 맞추기위해 위해\n",
      "Episode: 6/6 - Action 이후 x 값: 10 / 수행 action: Down / 다음상태 y 값: tensor([-1.9149]) / reward: 2.61\n",
      "loss: 0.1784823089838028\n",
      "성공! 4 / 7\n",
      "\n",
      "setPoint (특정 y값): 187 을 맞추기위해 위해\n",
      "Episode: 7/0 - Action 이후 x 값: 51 / 수행 action: Down / 다음상태 y 값: tensor([-2.3800]) / reward: 2.1\n",
      "loss: 0.2179437279701233\n",
      "성공! 5 / 8\n",
      "\n",
      "setPoint (특정 y값): 171 을 맞추기위해 위해\n",
      "Episode: 8/51 - Action 이후 x 값: 47 / 수행 action: Down / 다음상태 y 값: tensor([-4.3834]) / reward: 1.14\n",
      "loss: 0.21416568756103516\n",
      "성공! 6 / 9\n",
      "\n",
      "setPoint (특정 y값): -7 을 맞추기위해 위해\n",
      "Episode: 9/84 - Action 이후 x 값: -4 / 수행 action: Down / 다음상태 y 값: tensor([-3.9269]) / reward: 1.27\n",
      "loss: 0.18356162309646606\n",
      "성공! 7 / 10\n",
      "\n",
      "setPoint (특정 y값): 321 을 맞추기위해 위해\n",
      "Episode: 10/140 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.2378203570842743\n",
      "실패! 4 / 11\n",
      "\n",
      "setPoint (특정 y값): 75 을 맞추기위해 위해\n",
      "Episode: 11/76 - Action 이후 x 값: 19 / 수행 action: Down / 다음상태 y 값: tensor([-2.4073]) / reward: 2.08\n",
      "loss: 0.15592201054096222\n",
      "성공! 8 / 12\n",
      "\n",
      "setPoint (특정 y값): 112 을 맞추기위해 위해\n",
      "Episode: 12/15 - Action 이후 x 값: 30 / 수행 action: Down / 다음상태 y 값: tensor([-3.8979]) / reward: 1.28\n",
      "loss: 0.29045405983924866\n",
      "성공! 9 / 13\n",
      "\n",
      "setPoint (특정 y값): 262 을 맞추기위해 위해\n",
      "Episode: 13/32 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.3475663363933563\n",
      "실패! 5 / 14\n",
      "\n",
      "setPoint (특정 y값): 76 을 맞추기위해 위해\n",
      "Episode: 14/13 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.2905985116958618\n",
      "실패! 6 / 15\n",
      "\n",
      "setPoint (특정 y값): 19 을 맞추기위해 위해\n",
      "Episode: 15/33 - Action 이후 x 값: 3 / 수행 action: Down / 다음상태 y 값: tensor([-2.4209]) / reward: 2.07\n",
      "loss: 0.26797300577163696\n",
      "성공! 10 / 16\n",
      "\n",
      "setPoint (특정 y값): 147 을 맞추기위해 위해\n",
      "Episode: 16/9 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.2181338369846344\n",
      "실패! 7 / 17\n",
      "\n",
      "setPoint (특정 y값): 75 을 맞추기위해 위해\n",
      "Episode: 17/0 - Action 이후 x 값: 19 / 수행 action: Down / 다음상태 y 값: tensor([-2.4073]) / reward: 2.08\n",
      "loss: 0.19511105120182037\n",
      "성공! 11 / 18\n",
      "\n",
      "setPoint (특정 y값): 230 을 맞추기위해 위해\n",
      "Episode: 18/2 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.21939483284950256\n",
      "실패! 8 / 19\n",
      "\n",
      "setPoint (특정 y값): 237 을 맞추기위해 위해\n",
      "Episode: 19/35 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.23925632238388062\n",
      "실패! 9 / 20\n",
      "\n",
      "setPoint (특정 y값): 242 을 맞추기위해 위해\n",
      "Episode: 20/57 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.23076370358467102\n",
      "실패! 10 / 21\n",
      "\n",
      "setPoint (특정 y값): -4 을 맞추기위해 위해\n",
      "Episode: 21/73 - Action 이후 x 값: -3 / 수행 action: Down / 다음상태 y 값: tensor([-4.4260]) / reward: 1.13\n",
      "loss: 0.2366754114627838\n",
      "성공! 12 / 22\n",
      "\n",
      "setPoint (특정 y값): 273 을 맞추기위해 위해\n",
      "Episode: 22/20 - Action 이후 x 값: 76 / 수행 action: Down / 다음상태 y 값: tensor([-3.8587]) / reward: 1.3\n",
      "loss: 0.14068347215652466\n",
      "성공! 13 / 23\n",
      "\n",
      "setPoint (특정 y값): 265 을 맞추기위해 위해\n",
      "Episode: 23/90 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.24224251508712769\n",
      "실패! 11 / 24\n",
      "\n",
      "setPoint (특정 y값): 246 을 맞추기위해 위해\n",
      "Episode: 24/39 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.24751922488212585\n",
      "실패! 12 / 25\n",
      "\n",
      "setPoint (특정 y값): 225 을 맞추기위해 위해\n",
      "Episode: 25/8 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.1913129985332489\n",
      "실패! 13 / 26\n",
      "\n",
      "setPoint (특정 y값): 233 을 맞추기위해 위해\n",
      "Episode: 26/8 - Action 이후 x 값: 64 / 수행 action: Down / 다음상태 y 값: tensor([-1.8689]) / reward: 2.68\n",
      "loss: 0.21878889203071594\n",
      "성공! 14 / 27\n",
      "\n",
      "setPoint (특정 y값): -8 을 맞추기위해 위해\n",
      "Episode: 27/7 - Action 이후 x 값: -4 / 수행 action: Down / 다음상태 y 값: tensor([-4.9269]) / reward: 1.01\n",
      "loss: 0.19866545498371124\n",
      "성공! 15 / 28\n",
      "\n",
      "setPoint (특정 y값): 332 을 맞추기위해 위해\n",
      "Episode: 28/22 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.1598484218120575\n",
      "실패! 14 / 29\n",
      "\n",
      "setPoint (특정 y값): 6 을 맞추기위해 위해\n",
      "Episode: 29/1 - Action 이후 x 값: 0 / 수행 action: Down / 다음상태 y 값: tensor([-4.9235]) / reward: 1.02\n",
      "loss: 0.22027751803398132\n",
      "성공! 16 / 30\n",
      "\n",
      "setPoint (특정 y값): 212 을 맞추기위해 위해\n",
      "Episode: 30/24 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.29422125220298767\n",
      "실패! 15 / 31\n",
      "\n",
      "setPoint (특정 y값): 45 을 맞추기위해 위해\n",
      "Episode: 31/0 - Action 이후 x 값: 9 / 수행 action: Down / 다음상태 y 값: tensor([2.5842]) / reward: 1.93\n",
      "loss: 0.12647020816802979\n",
      "성공! 17 / 32\n",
      "\n",
      "setPoint (특정 y값): 230 을 맞추기위해 위해\n",
      "Episode: 32/8 - Action 이후 x 값: 64 / 수행 action: Down / 다음상태 y 값: tensor([-4.8689]) / reward: 1.03\n",
      "loss: 0.2070179283618927\n",
      "성공! 18 / 33\n",
      "\n",
      "setPoint (특정 y값): 269 을 맞추기위해 위해\n",
      "Episode: 33/96 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.18637365102767944\n",
      "실패! 16 / 34\n",
      "\n",
      "setPoint (특정 y값): 118 을 맞추기위해 위해\n",
      "Episode: 34/67 - Action 이후 x 값: 32 / 수행 action: Down / 다음상태 y 값: tensor([-4.8962]) / reward: 1.02\n",
      "loss: 0.19713431596755981\n",
      "성공! 19 / 35\n",
      "\n",
      "setPoint (특정 y값): 205 을 맞추기위해 위해\n",
      "Episode: 35/16 - Action 이후 x 값: 56 / 수행 action: Down / 다음상태 y 값: tensor([-1.8757]) / reward: 2.67\n",
      "loss: 0.24570821225643158\n",
      "성공! 20 / 36\n",
      "\n",
      "setPoint (특정 y값): 295 을 맞추기위해 위해\n",
      "Episode: 36/117 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.2170519232749939\n",
      "실패! 17 / 37\n",
      "\n",
      "setPoint (특정 y값): 279 을 맞추기위해 위해\n",
      "Episode: 37/4 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.18186399340629578\n",
      "실패! 18 / 38\n",
      "\n",
      "setPoint (특정 y값): 171 을 맞추기위해 위해\n",
      "Episode: 38/8 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.21582269668579102\n",
      "실패! 19 / 39\n",
      "\n",
      "setPoint (특정 y값): 120 을 맞추기위해 위해\n",
      "Episode: 39/32 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.12678103148937225\n",
      "실패! 20 / 40\n",
      "\n",
      "setPoint (특정 y값): 208 을 맞추기위해 위해\n",
      "Episode: 40/4 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.21219682693481445\n",
      "실패! 21 / 41\n",
      "\n",
      "setPoint (특정 y값): -18 을 맞추기위해 위해\n",
      "Episode: 41/63 - Action 이후 x 값: -7 / 수행 action: Down / 다음상태 y 값: tensor([-4.4294]) / reward: 1.13\n",
      "loss: 0.21524958312511444\n",
      "성공! 21 / 42\n",
      "\n",
      "setPoint (특정 y값): 140 을 맞추기위해 위해\n",
      "Episode: 42/12 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.26217177510261536\n",
      "실패! 22 / 43\n",
      "\n",
      "setPoint (특정 y값): 109 을 맞추기위해 위해\n",
      "Episode: 43/74 - Action 이후 x 값: 29 / 수행 action: Down / 다음상태 y 값: tensor([-3.3987]) / reward: 1.47\n",
      "loss: 0.22426703572273254\n",
      "성공! 22 / 44\n",
      "\n",
      "setPoint (특정 y값): 335 을 맞추기위해 위해\n",
      "Episode: 44/294 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.11567314714193344\n",
      "실패! 23 / 45\n",
      "\n",
      "setPoint (특정 y값): 86 을 맞추기위해 위해\n",
      "Episode: 45/5 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.15177759528160095\n",
      "실패! 24 / 46\n",
      "\n",
      "setPoint (특정 y값): 215 을 맞추기위해 위해\n",
      "Episode: 46/77 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.1740785539150238\n",
      "실패! 25 / 47\n",
      "\n",
      "setPoint (특정 y값): 318 을 맞추기위해 위해\n",
      "Episode: 47/447 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.1703234165906906\n",
      "실패! 26 / 48\n",
      "\n",
      "setPoint (특정 y값): 214 을 맞추기위해 위해\n",
      "Episode: 48/17 - Action 이후 x 값: 59 / 수행 action: Down / 다음상태 y 값: tensor([-3.3732]) / reward: 1.48\n",
      "loss: 0.1513933688402176\n",
      "성공! 23 / 49\n",
      "\n",
      "setPoint (특정 y값): 327 을 맞추기위해 위해\n",
      "Episode: 49/5 - Action 이후 x 값: 91 / 수행 action: Down / 다음상태 y 값: tensor([-2.3459]) / reward: 2.13\n",
      "loss: 0.22506605088710785\n",
      "성공! 24 / 50\n",
      "\n",
      "setPoint (특정 y값): 225 을 맞추기위해 위해\n",
      "Episode: 50/17 - Action 이후 x 값: 62 / 수행 action: Down / 다음상태 y 값: tensor([-2.8706]) / reward: 1.74\n",
      "loss: 0.20592689514160156\n",
      "성공! 25 / 51\n",
      "\n",
      "setPoint (특정 y값): 103 을 맞추기위해 위해\n",
      "Episode: 51/0 - Action 이후 x 값: 27 / 수행 action: Down / 다음상태 y 값: tensor([-2.4004]) / reward: 2.08\n",
      "loss: 0.2237844616174698\n",
      "성공! 26 / 52\n",
      "\n",
      "setPoint (특정 y값): -22 을 맞추기위해 위해\n",
      "Episode: 52/21 - Action 이후 x 값: -8 / 수행 action: Down / 다음상태 y 값: tensor([-4.9303]) / reward: 1.01\n",
      "loss: 0.1882399618625641\n",
      "성공! 27 / 53\n",
      "\n",
      "setPoint (특정 y값): 272 을 맞추기위해 위해\n",
      "Episode: 53/50 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.21160048246383667\n",
      "실패! 27 / 54\n",
      "\n",
      "setPoint (특정 y값): 80 을 맞추기위해 위해\n",
      "Episode: 54/72 - Action 이후 x 값: 21 / 수행 action: Down / 다음상태 y 값: tensor([-4.4056]) / reward: 1.13\n",
      "loss: 0.26870039105415344\n",
      "성공! 28 / 55\n",
      "\n",
      "setPoint (특정 y값): 302 을 맞추기위해 위해\n",
      "Episode: 55/43 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.2508988380432129\n",
      "실패! 28 / 56\n",
      "\n",
      "setPoint (특정 y값): 302 을 맞추기위해 위해\n",
      "Episode: 56/225 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.19367435574531555\n",
      "실패! 29 / 57\n",
      "\n",
      "setPoint (특정 y값): 142 을 맞추기위해 위해\n",
      "Episode: 57/14 - Action 이후 x 값: 38 / 수행 action: Down / 다음상태 y 값: tensor([-1.8911]) / reward: 2.64\n",
      "loss: 0.17610715329647064\n",
      "성공! 29 / 58\n",
      "\n",
      "setPoint (특정 y값): 176 을 맞추기위해 위해\n",
      "Episode: 58/17 - Action 이후 x 값: 48 / 수행 action: Down / 다음상태 y 값: tensor([-2.8825]) / reward: 1.73\n",
      "loss: 0.22671353816986084\n",
      "성공! 30 / 59\n",
      "\n",
      "setPoint (특정 y값): 62 을 맞추기위해 위해\n",
      "Episode: 59/45 - Action 이후 x 값: 16 / 수행 action: Down / 다음상태 y 값: tensor([-4.9098]) / reward: 1.02\n",
      "loss: 0.18536797165870667\n",
      "성공! 31 / 60\n",
      "\n",
      "setPoint (특정 y값): 58 을 맞추기위해 위해\n",
      "Episode: 60/17 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.24155282974243164\n",
      "실패! 30 / 61\n",
      "\n",
      "setPoint (특정 y값): 144 을 맞추기위해 위해\n",
      "Episode: 61/1316 - Action 이후 x 값: 37 / 수행 action: Up / 다음상태 y 값: tensor([3.6081]) / reward: 1.39\n",
      "loss: 0.19520637392997742\n",
      "성공! 32 / 62\n",
      "\n",
      "setPoint (특정 y값): 344 을 맞추기위해 위해\n",
      "Episode: 62/52 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.25242698192596436\n",
      "실패! 31 / 63\n",
      "\n",
      "setPoint (특정 y값): 118 을 맞추기위해 위해\n",
      "Episode: 63/39 - Action 이후 x 값: 32 / 수행 action: Down / 다음상태 y 값: tensor([-4.8962]) / reward: 1.02\n",
      "loss: 0.26392096281051636\n",
      "성공! 33 / 64\n",
      "\n",
      "setPoint (특정 y값): 51 을 맞추기위해 위해\n",
      "Episode: 64/70 - Action 이후 x 값: 12 / 수행 action: Down / 다음상태 y 값: tensor([-1.9132]) / reward: 2.61\n",
      "loss: 0.3297904133796692\n",
      "성공! 34 / 65\n",
      "\n",
      "setPoint (특정 y값): 273 을 맞추기위해 위해\n",
      "Episode: 65/80 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.3510458469390869\n",
      "실패! 32 / 66\n",
      "\n",
      "setPoint (특정 y값): 96 을 맞추기위해 위해\n",
      "Episode: 66/61 - Action 이후 x 값: 25 / 수행 action: Down / 다음상태 y 값: tensor([-2.4021]) / reward: 2.08\n",
      "loss: 0.23603539168834686\n",
      "성공! 35 / 67\n",
      "\n",
      "setPoint (특정 y값): 175 을 맞추기위해 위해\n",
      "Episode: 67/52 - Action 이후 x 값: 48 / 수행 action: Down / 다음상태 y 값: tensor([-3.8825]) / reward: 1.29\n",
      "loss: 0.28698405623435974\n",
      "성공! 36 / 68\n",
      "\n",
      "setPoint (특정 y값): 201 을 맞추기위해 위해\n",
      "Episode: 68/26 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.328313946723938\n",
      "실패! 33 / 69\n",
      "\n",
      "setPoint (특정 y값): 65 을 맞추기위해 위해\n",
      "Episode: 69/24 - Action 이후 x 값: 16 / 수행 action: Down / 다음상태 y 값: tensor([-1.9098]) / reward: 2.62\n",
      "loss: 0.36907708644866943\n",
      "성공! 37 / 70\n",
      "\n",
      "setPoint (특정 y값): -8 을 맞추기위해 위해\n",
      "Episode: 70/28 - Action 이후 x 값: -4 / 수행 action: Down / 다음상태 y 값: tensor([-4.9269]) / reward: 1.01\n",
      "loss: 0.35068318247795105\n",
      "성공! 38 / 71\n",
      "\n",
      "setPoint (특정 y값): 101 을 맞추기위해 위해\n",
      "Episode: 71/33 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.18387754261493683\n",
      "실패! 34 / 72\n",
      "\n",
      "setPoint (특정 y값): 143 을 맞추기위해 위해\n",
      "Episode: 72/29 - Action 이후 x 값: 39 / 수행 action: Down / 다음상태 y 값: tensor([-4.3902]) / reward: 1.14\n",
      "loss: 0.2871248722076416\n",
      "성공! 39 / 73\n",
      "\n",
      "setPoint (특정 y값): 151 을 맞추기위해 위해\n",
      "Episode: 73/5 - Action 이후 x 값: 41 / 수행 action: Down / 다음상태 y 값: tensor([-3.3885]) / reward: 1.48\n",
      "loss: 0.22021186351776123\n",
      "성공! 40 / 74\n",
      "\n",
      "setPoint (특정 y값): 103 을 맞추기위해 위해\n",
      "Episode: 74/61 - Action 이후 x 값: 27 / 수행 action: Down / 다음상태 y 값: tensor([-2.4004]) / reward: 2.08\n",
      "loss: 0.2159913182258606\n",
      "성공! 41 / 75\n",
      "\n",
      "setPoint (특정 y값): 226 을 맞추기위해 위해\n",
      "Episode: 75/51 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.18797308206558228\n",
      "실패! 35 / 76\n",
      "\n",
      "setPoint (특정 y값): 81 을 맞추기위해 위해\n",
      "Episode: 76/35 - Action 이후 x 값: 21 / 수행 action: Down / 다음상태 y 값: tensor([-3.4056]) / reward: 1.47\n",
      "loss: 0.31636181473731995\n",
      "성공! 42 / 77\n",
      "\n",
      "setPoint (특정 y값): 208 을 맞추기위해 위해\n",
      "Episode: 77/0 - Action 이후 x 값: 57 / 수행 action: Down / 다음상태 y 값: tensor([-2.3749]) / reward: 2.11\n",
      "loss: 0.24924468994140625\n",
      "성공! 43 / 78\n",
      "\n",
      "setPoint (특정 y값): 194 을 맞추기위해 위해\n",
      "Episode: 78/39 - Action 이후 x 값: 53 / 수행 action: Down / 다음상태 y 값: tensor([-2.3783]) / reward: 2.1\n",
      "loss: 0.2665887475013733\n",
      "성공! 44 / 79\n",
      "\n",
      "setPoint (특정 y값): 7 을 맞추기위해 위해\n",
      "Episode: 79/19 - Action 이후 x 값: 0 / 수행 action: Down / 다음상태 y 값: tensor([-3.9235]) / reward: 1.27\n",
      "loss: 0.252882719039917\n",
      "성공! 45 / 80\n",
      "\n",
      "setPoint (특정 y값): 71 을 맞추기위해 위해\n",
      "Episode: 80/22 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.2734453082084656\n",
      "실패! 36 / 81\n",
      "\n",
      "setPoint (특정 y값): 21 을 맞추기위해 위해\n",
      "Episode: 81/67 - Action 이후 x 값: 4 / 수행 action: Down / 다음상태 y 값: tensor([-3.9200]) / reward: 1.28\n",
      "loss: 0.2218204140663147\n",
      "성공! 46 / 82\n",
      "\n",
      "setPoint (특정 y값): 194 을 맞추기위해 위해\n",
      "Episode: 82/0 - Action 이후 x 값: 51 / 수행 action: Down / 다음상태 y 값: tensor([4.6200]) / reward: 1.08\n",
      "loss: 0.2432517260313034\n",
      "성공! 47 / 83\n",
      "\n",
      "setPoint (특정 y값): 43 을 맞추기위해 위해\n",
      "Episode: 83/17 - Action 이후 x 값: 10 / 수행 action: Down / 다음상태 y 값: tensor([-2.9149]) / reward: 1.72\n",
      "loss: 0.26153764128685\n",
      "성공! 48 / 84\n",
      "\n",
      "setPoint (특정 y값): -23 을 맞추기위해 위해\n",
      "Episode: 84/58 - Action 이후 x 값: -9 / 수행 action: Down / 다음상태 y 값: tensor([-2.4311]) / reward: 2.06\n",
      "loss: 0.20476220548152924\n",
      "성공! 49 / 85\n",
      "\n",
      "setPoint (특정 y값): 11 을 맞추기위해 위해\n",
      "Episode: 85/95 - Action 이후 x 값: 1 / 수행 action: Down / 다음상태 y 값: tensor([-3.4226]) / reward: 1.46\n",
      "loss: 0.2504841983318329\n",
      "성공! 50 / 86\n",
      "\n",
      "setPoint (특정 y값): 93 을 맞추기위해 위해\n",
      "Episode: 86/81 - Action 이후 x 값: 24 / 수행 action: Down / 다음상태 y 값: tensor([-1.9030]) / reward: 2.63\n",
      "loss: 0.2526852488517761\n",
      "성공! 51 / 87\n",
      "\n",
      "setPoint (특정 y값): 52 을 맞추기위해 위해\n",
      "Episode: 87/0 - Action 이후 x 값: 12 / 수행 action: Down / 다음상태 y 값: tensor([-0.9132]) / reward: 5.48\n",
      "loss: 0.26376453042030334\n",
      "성공! 52 / 88\n",
      "\n",
      "setPoint (특정 y값): 117 을 맞추기위해 위해\n",
      "Episode: 88/27 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.2638270854949951\n",
      "실패! 37 / 89\n",
      "\n",
      "setPoint (특정 y값): 334 을 맞추기위해 위해\n",
      "Episode: 89/11 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.24677865207195282\n",
      "실패! 38 / 90\n",
      "\n",
      "setPoint (특정 y값): 33 을 맞추기위해 위해\n",
      "Episode: 90/93 - Action 이후 x 값: 7 / 수행 action: Down / 다음상태 y 값: tensor([-2.4175]) / reward: 2.07\n",
      "loss: 0.3017705976963043\n",
      "성공! 53 / 91\n",
      "\n",
      "setPoint (특정 y값): 149 을 맞추기위해 위해\n",
      "Episode: 91/30 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.20863091945648193\n",
      "실패! 39 / 92\n",
      "\n",
      "setPoint (특정 y값): 155 을 맞추기위해 위해\n",
      "Episode: 92/15 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.23783686757087708\n",
      "실패! 40 / 93\n",
      "\n",
      "setPoint (특정 y값): 248 을 맞추기위해 위해\n",
      "Episode: 93/62 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.2362876832485199\n",
      "실패! 41 / 94\n",
      "\n",
      "setPoint (특정 y값): 351 을 맞추기위해 위해\n",
      "Episode: 94/114 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.22817355394363403\n",
      "실패! 42 / 95\n",
      "\n",
      "setPoint (특정 y값): 110 을 맞추기위해 위해\n",
      "Episode: 95/15 - Action 이후 x 값: -11 / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: 0.1790345311164856\n",
      "실패! 43 / 96\n",
      "\n",
      "setPoint (특정 y값): -7 을 맞추기위해 위해\n",
      "Episode: 96/83 - Action 이후 x 값: -4 / 수행 action: Down / 다음상태 y 값: tensor([-3.9269]) / reward: 1.27\n",
      "loss: 0.13750991225242615\n",
      "성공! 54 / 97\n",
      "\n",
      "setPoint (특정 y값): -5 을 맞추기위해 위해\n",
      "Episode: 97/20 - Action 이후 x 값: -4 / 수행 action: Down / 다음상태 y 값: tensor([-1.9269]) / reward: 2.59\n",
      "loss: 0.26306891441345215\n",
      "성공! 55 / 98\n",
      "\n",
      "setPoint (특정 y값): -11 을 맞추기위해 위해\n",
      "Episode: 98/73 - Action 이후 x 값: -5 / 수행 action: Down / 다음상태 y 값: tensor([-4.4277]) / reward: 1.13\n",
      "loss: 0.21907532215118408\n",
      "성공! 56 / 99\n",
      "\n",
      "setPoint (특정 y값): 119 을 맞추기위해 위해\n",
      "Episode: 99/58 - Action 이후 x 값: 32 / 수행 action: Down / 다음상태 y 값: tensor([-3.8962]) / reward: 1.28\n",
      "loss: 0.25182193517684937\n",
      "성공! 57 / 100\n",
      "\n",
      "Complete 57\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "success_cnt = 0\n",
    "fail_cnt = 0\n",
    "num_episodes = 100\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    epMemory = list()\n",
    "    z = random.randrange(-10, 100)\n",
    "    setPoint = random.randrange(math.floor(min(data['y'])), math.ceil(max(data['y'])))\n",
    "    env = Environment(z=z, setPoint=setPoint)\n",
    "    state = env.reset()\n",
    "    print(\"setPoint (특정 y값):\", setPoint, \"을 맞추기위해 위해\")\n",
    "    for t in count():\n",
    "        # 행동 결정\n",
    "        action = select_action(torch.tensor([state]).float())\n",
    "        next_state, reward, done = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        # 메모리에 경험 저장\n",
    "        epMemory.append([state, action, next_state, reward])\n",
    "        \n",
    "        # 다음 상태로 이동\n",
    "        state = next_state\n",
    "        \n",
    "        loss = optimize_model()\n",
    "        \n",
    "        msg = f\"Episode: {i_episode}/{t} - Action 이후 x 값: {env.render()} / 수행 action: {Action(action)} / 다음상태 y 값: {next_state} / reward: {round(reward.item(),2)}\"\n",
    "        # print(msg)\n",
    "        # print('loss:',loss)\n",
    "        \n",
    "        if done:\n",
    "            print(msg)\n",
    "            print('loss:',loss)\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations(is_ipython=False)\n",
    "            if env.render() >= -10 and env.render() <= 100:\n",
    "                success_cnt+=1\n",
    "                _ = [memory.push(epMemory[i][0], epMemory[i][1], epMemory[i][2], epMemory[i][3]) for i in range(len(epMemory))]\n",
    "                print(f\"성공! {success_cnt} / {success_cnt+fail_cnt}\")\n",
    "                \n",
    "            else:\n",
    "                fail_cnt += 1\n",
    "                print(f\"실패! {fail_cnt} / {success_cnt+fail_cnt}\")\n",
    "            print(\"\")\n",
    "            break\n",
    "\n",
    "        if t >= 10000:\n",
    "            print(\"중단!\")\n",
    "            break\n",
    "        \n",
    "    # 주기적 네트워크 업데이트, 타겟 네트워크 업데이트  \n",
    "    if i_episode % TARGET_UPDATE == 0:  # 10번에 한버씩 \n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print(f\"Complete {success_cnt}\")\n",
    "plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "episode를 수행하기 위해 현재 x(코드에서는 z)값과 setPoint 모두 random하게 생성하고, 현 state에서 reward를 최대화하기 위해 가장 좋은 다음 action을 취하는 방법을 학습한다. \n",
    "\n",
    "성공한 episode만 memory에 저장하여 학습하였다. (Pytorch Tutorial에서는 모든 episode를 다 저장한다.)\n",
    "\n",
    "또한, 한 episode에서 step이 10,000번이 넘어가면 중단하고 다음 episode를 새롭게 시작하도록 구현하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 검증 및 Simulation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이 학습된 모델을 실제 환경에 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setPoint -25 을 맞추기 위해\n",
      "0 - action 이후 x 값: 57에서 Down 하면 setPoint(y) 값은 tensor([-235.3749]) / reward: 0.02\n",
      "1 - action 이후 x 값: 56에서 Down 하면 setPoint(y) 값은 tensor([-231.8757]) / reward: 0.02\n",
      "2 - action 이후 x 값: 57에서 Up 하면 setPoint(y) 값은 tensor([-235.3749]) / reward: 0.02\n",
      "3 - action 이후 x 값: 56에서 Down 하면 setPoint(y) 값은 tensor([-231.8757]) / reward: 0.02\n",
      "4 - action 이후 x 값: 55에서 Down 하면 setPoint(y) 값은 tensor([-228.3766]) / reward: 0.02\n",
      "5 - action 이후 x 값: 54에서 Down 하면 setPoint(y) 값은 tensor([-224.8774]) / reward: 0.02\n",
      "6 - action 이후 x 값: 53에서 Down 하면 setPoint(y) 값은 tensor([-221.3783]) / reward: 0.02\n",
      "7 - action 이후 x 값: 52에서 Down 하면 setPoint(y) 값은 tensor([-217.8791]) / reward: 0.02\n",
      "8 - action 이후 x 값: 51에서 Down 하면 setPoint(y) 값은 tensor([-214.3800]) / reward: 0.02\n",
      "9 - action 이후 x 값: 52에서 Up 하면 setPoint(y) 값은 tensor([-217.8791]) / reward: 0.02\n",
      "10 - action 이후 x 값: 51에서 Down 하면 setPoint(y) 값은 tensor([-214.3800]) / reward: 0.02\n",
      "11 - action 이후 x 값: 50에서 Down 하면 setPoint(y) 값은 tensor([-210.8808]) / reward: 0.02\n",
      "12 - action 이후 x 값: 49에서 Down 하면 setPoint(y) 값은 tensor([-207.3817]) / reward: 0.02\n",
      "13 - action 이후 x 값: 48에서 Down 하면 setPoint(y) 값은 tensor([-203.8825]) / reward: 0.02\n",
      "14 - action 이후 x 값: 47에서 Down 하면 setPoint(y) 값은 tensor([-200.3834]) / reward: 0.02\n",
      "15 - action 이후 x 값: 46에서 Down 하면 setPoint(y) 값은 tensor([-196.8842]) / reward: 0.03\n",
      "16 - action 이후 x 값: 45에서 Down 하면 setPoint(y) 값은 tensor([-193.3851]) / reward: 0.03\n",
      "17 - action 이후 x 값: 44에서 Down 하면 setPoint(y) 값은 tensor([-189.8859]) / reward: 0.03\n",
      "18 - action 이후 x 값: 43에서 Down 하면 setPoint(y) 값은 tensor([-186.3868]) / reward: 0.03\n",
      "19 - action 이후 x 값: 42에서 Down 하면 setPoint(y) 값은 tensor([-182.8876]) / reward: 0.03\n",
      "20 - action 이후 x 값: 41에서 Down 하면 setPoint(y) 값은 tensor([-179.3885]) / reward: 0.03\n",
      "21 - action 이후 x 값: 40에서 Down 하면 setPoint(y) 값은 tensor([-175.8894]) / reward: 0.03\n",
      "22 - action 이후 x 값: 39에서 Down 하면 setPoint(y) 값은 tensor([-172.3902]) / reward: 0.03\n",
      "23 - action 이후 x 값: 38에서 Down 하면 setPoint(y) 값은 tensor([-168.8911]) / reward: 0.03\n",
      "24 - action 이후 x 값: 37에서 Down 하면 setPoint(y) 값은 tensor([-165.3919]) / reward: 0.03\n",
      "25 - action 이후 x 값: 36에서 Down 하면 setPoint(y) 값은 tensor([-161.8928]) / reward: 0.03\n",
      "26 - action 이후 x 값: 35에서 Down 하면 setPoint(y) 값은 tensor([-158.3936]) / reward: 0.03\n",
      "27 - action 이후 x 값: 34에서 Down 하면 setPoint(y) 값은 tensor([-154.8945]) / reward: 0.03\n",
      "28 - action 이후 x 값: 33에서 Down 하면 setPoint(y) 값은 tensor([-151.3953]) / reward: 0.03\n",
      "29 - action 이후 x 값: 32에서 Down 하면 setPoint(y) 값은 tensor([-147.8962]) / reward: 0.03\n",
      "30 - action 이후 x 값: 31에서 Down 하면 setPoint(y) 값은 tensor([-144.3970]) / reward: 0.03\n",
      "31 - action 이후 x 값: 30에서 Down 하면 setPoint(y) 값은 tensor([-140.8979]) / reward: 0.04\n",
      "32 - action 이후 x 값: 29에서 Down 하면 setPoint(y) 값은 tensor([-137.3987]) / reward: 0.04\n",
      "33 - action 이후 x 값: 28에서 Down 하면 setPoint(y) 값은 tensor([-133.8996]) / reward: 0.04\n",
      "34 - action 이후 x 값: 27에서 Down 하면 setPoint(y) 값은 tensor([-130.4004]) / reward: 0.04\n",
      "35 - action 이후 x 값: 26에서 Down 하면 setPoint(y) 값은 tensor([-126.9013]) / reward: 0.04\n",
      "36 - action 이후 x 값: 25에서 Down 하면 setPoint(y) 값은 tensor([-123.4021]) / reward: 0.04\n",
      "37 - action 이후 x 값: 24에서 Down 하면 setPoint(y) 값은 tensor([-119.9030]) / reward: 0.04\n",
      "38 - action 이후 x 값: 23에서 Down 하면 setPoint(y) 값은 tensor([-116.4038]) / reward: 0.04\n",
      "39 - action 이후 x 값: 22에서 Down 하면 setPoint(y) 값은 tensor([-112.9047]) / reward: 0.04\n",
      "40 - action 이후 x 값: 21에서 Down 하면 setPoint(y) 값은 tensor([-109.4056]) / reward: 0.05\n",
      "41 - action 이후 x 값: 20에서 Down 하면 setPoint(y) 값은 tensor([-105.9064]) / reward: 0.05\n",
      "42 - action 이후 x 값: 19에서 Down 하면 setPoint(y) 값은 tensor([-102.4073]) / reward: 0.05\n",
      "43 - action 이후 x 값: 18에서 Down 하면 setPoint(y) 값은 tensor([-98.9081]) / reward: 0.05\n",
      "44 - action 이후 x 값: 19에서 Up 하면 setPoint(y) 값은 tensor([-102.4073]) / reward: 0.05\n",
      "45 - action 이후 x 값: 18에서 Down 하면 setPoint(y) 값은 tensor([-98.9081]) / reward: 0.05\n",
      "46 - action 이후 x 값: 17에서 Down 하면 setPoint(y) 값은 tensor([-95.4090]) / reward: 0.05\n",
      "47 - action 이후 x 값: 16에서 Down 하면 setPoint(y) 값은 tensor([-91.9098]) / reward: 0.05\n",
      "48 - action 이후 x 값: 15에서 Down 하면 setPoint(y) 값은 tensor([-88.4107]) / reward: 0.06\n",
      "49 - action 이후 x 값: 14에서 Down 하면 setPoint(y) 값은 tensor([-84.9115]) / reward: 0.06\n",
      "50 - action 이후 x 값: 13에서 Down 하면 setPoint(y) 값은 tensor([-81.4124]) / reward: 0.06\n",
      "51 - action 이후 x 값: 12에서 Down 하면 setPoint(y) 값은 tensor([-77.9132]) / reward: 0.06\n",
      "52 - action 이후 x 값: 11에서 Down 하면 setPoint(y) 값은 tensor([-74.4141]) / reward: 0.07\n",
      "53 - action 이후 x 값: 10에서 Down 하면 setPoint(y) 값은 tensor([-70.9149]) / reward: 0.07\n",
      "54 - action 이후 x 값: 9에서 Down 하면 setPoint(y) 값은 tensor([-67.4158]) / reward: 0.07\n",
      "55 - action 이후 x 값: 8에서 Down 하면 setPoint(y) 값은 tensor([-63.9166]) / reward: 0.08\n",
      "56 - action 이후 x 값: 7에서 Down 하면 setPoint(y) 값은 tensor([-60.4175]) / reward: 0.08\n",
      "57 - action 이후 x 값: 6에서 Down 하면 setPoint(y) 값은 tensor([-56.9183]) / reward: 0.09\n",
      "58 - action 이후 x 값: 5에서 Down 하면 setPoint(y) 값은 tensor([-53.4192]) / reward: 0.09\n",
      "59 - action 이후 x 값: 4에서 Down 하면 setPoint(y) 값은 tensor([-49.9200]) / reward: 0.1\n",
      "60 - action 이후 x 값: 3에서 Down 하면 setPoint(y) 값은 tensor([-46.4209]) / reward: 0.11\n",
      "61 - action 이후 x 값: 2에서 Down 하면 setPoint(y) 값은 tensor([-42.9217]) / reward: 0.12\n",
      "62 - action 이후 x 값: 1에서 Down 하면 setPoint(y) 값은 tensor([-39.4226]) / reward: 0.13\n",
      "63 - action 이후 x 값: 0에서 Down 하면 setPoint(y) 값은 tensor([-35.9235]) / reward: 0.14\n",
      "64 - action 이후 x 값: -1에서 Down 하면 setPoint(y) 값은 tensor([-32.4243]) / reward: 0.15\n",
      "65 - action 이후 x 값: -2에서 Down 하면 setPoint(y) 값은 tensor([-28.9252]) / reward: 0.17\n",
      "66 - action 이후 x 값: -3에서 Down 하면 setPoint(y) 값은 tensor([-25.4260]) / reward: 0.2\n",
      "67 - action 이후 x 값: -4에서 Down 하면 setPoint(y) 값은 tensor([-21.9269]) / reward: 0.23\n",
      "68 - action 이후 x 값: -5에서 Down 하면 setPoint(y) 값은 tensor([-18.4277]) / reward: 0.27\n",
      "69 - action 이후 x 값: -6에서 Down 하면 setPoint(y) 값은 tensor([-14.9286]) / reward: 0.33\n",
      "70 - action 이후 x 값: -7에서 Down 하면 setPoint(y) 값은 tensor([-11.4294]) / reward: 0.44\n",
      "71 - action 이후 x 값: -8에서 Down 하면 setPoint(y) 값은 tensor([-7.9303]) / reward: 0.63\n",
      "72 - action 이후 x 값: -9에서 Down 하면 setPoint(y) 값은 tensor([-4.4311]) / reward: 1.13\n",
      "성공!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = random.randrange(-10, 100)\n",
    "setPoint = random.randrange(math.floor(min(data['y'])), math.ceil(max(data['y'])), math.ceil(max(data['y'])))\n",
    "\n",
    "env = Environment(z=z, setPoint=setPoint)\n",
    "\n",
    "yPred = model.predict(np.array([z]).reshape(-1,1)).item()\n",
    "state = torch.tensor([setPoint - yPred])    # 지금 y 값\n",
    "# 이 state에서 setPoint가 되기 위해서 어떤 action을 취하라 \n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"setPoint\", setPoint, \"을 맞추기 위해\")\n",
    "    for t in count():\n",
    "        action = select_action(torch.tensor([state]).float())\n",
    "        next_state, reward, done = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        msg = f\"{t} - action 이후 x 값: {env.render()}에서 {Action(action)} 하면 setPoint(y) 값은 {next_state} / reward: {round(reward.item(),2)}\"\n",
    "        print(msg)\n",
    "\n",
    "        # 다음 상태로 이동 \n",
    "        state = next_state \n",
    "        \n",
    "        if done:\n",
    "            if env.render() >= -10 and env.render() <= 100:\n",
    "                print(\"성공!\")\n",
    "                target_input = env.render()\n",
    "            else:\n",
    "                print(\"실패!\")\n",
    "            print()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-20.568872387209495\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(np.array([target_input]).reshape(-1,1)).item()\n",
    "\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
