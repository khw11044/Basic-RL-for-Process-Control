{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 필요한 라이브러리들을 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 예제 데이터를 생성한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 품질 (Property, y)을 Setting한 Point에 맞추기 위해 온도 (Temperature, x)를 어떻게 운전해야하나?를 취적화 하는 것을 목적으로 하는 상황이며, x와 y는 아주 강한 선형성을 띈다고 가정한다.\n",
    "\n",
    "아래 생성한 데이터는 \"그동안 temperature (x)를 이런식으로 운전했을 때, 제품의 품질 (y)는 이랬다\"라는 데이터라고 이해하면 된다.\n",
    "\n",
    "아래 그래프로 시각화 하였을 때, 온도(x)가 높을 수록 품질(y)는 상승한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예제 데이터 생성 \n",
    "import pandas as pd \n",
    "temp = [random.randint(-10,100) for _ in range(100)]\n",
    "property = [i*3.5 + 10 + random.random()*2 for i in temp]\n",
    "# property = [math.sin(math.pi * (i*10 / 180)) for i in temp]\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'x': temp,\n",
    "    'y':property\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>153.622622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6</td>\n",
       "      <td>-9.897396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50</td>\n",
       "      <td>185.660192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>97.922067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>98.492088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    x           y\n",
       "0  41  153.622622\n",
       "1  -6   -9.897396\n",
       "2  50  185.660192\n",
       "3  25   97.922067\n",
       "4  25   98.492088"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='x', ylabel='y'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot(x='x', y='y', kind='scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Environment 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "episode를 스스로 생성하기 위한 state, reward, action 등 environment를 정의한다. \n",
    "\n",
    "상황을 개발자가 원하는대로 정의하면 되지만, 공정의 경우 모든 episode 상황에 대한 결과가 어떨지 실제로 실험해보기에는 현실적으로 불가능에 가깝다.\n",
    "\n",
    "따라서, 관심있는 공정에서 x값이 ~할때, y값이 어떨지를 추정할 수 있는 x와 y의 Model이 필요하다.\n",
    "\n",
    "애초에 선형성이 강한 가상의 환경을 정의했으므로 Linear Regression 모델을 생성한다. \n",
    "\n",
    "(당연히 x와 y의 Linear Regression 모델을 알고 있다면 y값을 어떤 setting point로 만들기 위해 x를 어떤 값으로 해야할지는 굳이 강화학습과 같은 최적화 알고리즘을 하지 않아도 쉽게 계산이 가능하다. 그러나, 지금은 최대한 단순한 가상의 환경을 가정하여 진행한다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.4995349]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "hist = model.fit(np.array(temp).reshape(-1,1), np.array(property).reshape(-1,1))\n",
    "\n",
    "print(hist.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999716801278229"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(np.array(temp).reshape(-1,1), np.array(property).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model의 coef를 확인해보면 3.5에 근접한다. 애초에 x*3.5로 y를 만들었으니 그럴 수 밖에 없다. 학습이 아주 잘되었다.\n",
    "\n",
    "이제 위에서 생성한 모델을 기반으로 Environment Class를 정의한다. \n",
    "\n",
    "Environment Class에는 생성자 외에 세 개 function이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, z, setPoint):\n",
    "        self.z = z \n",
    "        self.setPoint = setPoint \n",
    "        self.terminated = False \n",
    "        self.state = None \n",
    "        \n",
    "    def reset(self):\n",
    "        self.yPred = model.predict(np.array([self.z]).reshape(-1,1)).item()\n",
    "        self.state = torch.tensor([self.setPoint - self.yPred])\n",
    "        #self.state = torch.tensor(self.state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        return self.state \n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0:\n",
    "            self.z -= 1\n",
    "        elif action == 1:\n",
    "            self.z += 1\n",
    "        \n",
    "        if self.z < -10 or self.z > 100:\n",
    "            reward = -100. \n",
    "            self.terminated = True \n",
    "            return None, torch.tensor([reward]), self.terminated \n",
    "        \n",
    "        self.yPred = model.predict(np.array([self.z]).reshape(-1,1)).item()\n",
    "        self.state = torch.tensor([self.setPoint - self.yPred])\n",
    "        #self.state = torch.tensor(self.state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        if abs(self.state) <= 5:\n",
    "            self.terminated = True \n",
    "        reward = 1 / abs(self.state) * 5\n",
    "        return self.state, reward, self.terminated \n",
    "    \n",
    "    def render(self):\n",
    "        return self.z, round(self.yPred,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 생성자를 보면 z와 setPoint를 초기값으로 받는다, z는 현재 x값을 의미하고 setPoint는 y값을 어떤 값으로 Setting하고 싶은지에 대한 값을 의미한다. \n",
    "\n",
    "\n",
    "**reset()** 은 매 episode마다 맨 처음 state를 reset하는 역할을 수행한다. 가능한 범위 내 random한 가상의 state 값을 return한다.\n",
    "\n",
    "state는 (setPoint - y)값으로 한다.\n",
    "\n",
    "첫, 이 state를 최소화하는 것이 목적이다.\n",
    " \n",
    "**step()** 은 action을 input으로 받아 현 state에서 action을 취한 다음 'state, reward, 종료인지 아니지'에 대한 정보를 return한다.\n",
    "\n",
    "본 과정에서 action은 0과 1로 정의한다.\n",
    "action이 0이면 x를 1만큼 낮추고, action이 1이면 x를 1만큼 키운다.\n",
    "\n",
    "reward는 1/abs(setPoint - y)로 정의한다.\n",
    "setting한 값과 현재 y값과의 차이가 작을수록 큰 reward를 발리한다.\n",
    "\n",
    "**y는 위에서 생성한 모델에 x를 넣어 prediction하여 계산한다. 이 과정을 위해 x와 y의 model을 만들었다고 이해하면 된다.**\n",
    "\n",
    "abs(setPoint - y)값이 5보다 작으면 최적화 되었다고 판단하여 episode를 종료한다.\n",
    "\n",
    "또한, action을 취한 후 x가 정의한 min ~ max range를 벗어나면 -100의 reward로 return하며 episode는 종료된다.\n",
    "\n",
    "**render()** 는 episode가 어떻게 진행되고 있는지 과정을 확인하기 위한 function으로, 꼭 필요한 function은 아니며 원하는 부분을 자유롭게 구현하면 된다. 여기서는 현재 x (z) 값을 return하도록 구현하였다.\n",
    "\n",
    "이렇듯 'state, action, reward, episode terminate 조건' 등 모든 환경의 상황과 최대한 유사하게, 효율적으로 모델이 최적화 할 수 있도록 정의되어야 하며 이 과정에는 Biz.domain이 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Reinforcement Model 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정의한 가상환경이 굉장히 단순하므로 Reinforcement Model도 아주 단순하게 구현한다.\n",
    "\n",
    "현 state를 입력받아 최적의 action을 return하는 역할을 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.linear1 = nn.Linear(1, 16, bias=True)\n",
    "        self.linear2 = nn.Linear(16, outputs, bias=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return torch.unsqueeze(F.log_softmax(x, dim=0), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Select Action 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "바로 위에서 정의한 reinforcement model에 기반하여 현 state를 입력으로 받아 다음 action을 return하는 function을 정의한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_actions = 2  # action 갯수\n",
    "policy_net = DQN(n_actions).to(device)\n",
    "target_net = DQN(n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "def select_action(state):\n",
    "    global steps_done \n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1]\n",
    "    else:\n",
    "        return torch.tensor([random.randrange(n_actions)], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "policy network에 의해서만 action을 취하다보면 local minimum에 빠질 수 있으므로 가끔 random한 선택을 하게 해준다.\n",
    "\n",
    "eps_threshold가 random한 어떤 값보다 작으면 policy network가 return한 action을 return하고, random한 어떤 값보다 크면 random한 action을 return한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Traning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 위에서 만든 모든 것들을 조합하여 학습을 진행한다.\n",
    "\n",
    "먼저 학습할 episode들을 저장할 memory를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque, namedtuple\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"transition 저장\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimize function도 정의한다.\n",
    "\n",
    "이 부분은 강화학습 Q-Learning을 이해하고 있어야 이해할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return \n",
    "    \n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.stack(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    \n",
    "    state_action_values = policy_net(state_batch).squeeze().gather(1, action_batch.unsqueeze(1))\n",
    "    \n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states.reshape(non_final_next_states.size()[0],1)).squeeze().max(1)[0].detach()\n",
    "    \n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    \n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1,1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Action(x):\n",
    "    if x.item() == 0:\n",
    "        return \"Down\"\n",
    "    else:\n",
    "        return \"Up\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 실제 episode를 진행하면서 강화학습 모델을 학습한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setPoint (특정 y값): 272 을 맞추기위해 위해\n",
      "Episode: 0/59 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 1 / 1\n",
      "\n",
      "setPoint (특정 y값): 330 을 맞추기위해 위해\n",
      "Episode: 1/48 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 2 / 2\n",
      "\n",
      "setPoint (특정 y값): 293 을 맞추기위해 위해\n",
      "Episode: 2/56 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 3 / 3\n",
      "\n",
      "setPoint (특정 y값): 220 을 맞추기위해 위해\n",
      "Episode: 3/82 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 4 / 4\n",
      "\n",
      "setPoint (특정 y값): 149 을 맞추기위해 위해\n",
      "Episode: 4/53 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 5 / 5\n",
      "\n",
      "setPoint (특정 y값): 118 을 맞추기위해 위해\n",
      "Episode: 5/51 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 6 / 6\n",
      "\n",
      "setPoint (특정 y값): 121 을 맞추기위해 위해\n",
      "Episode: 6/42 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 7 / 7\n",
      "\n",
      "setPoint (특정 y값): 275 을 맞추기위해 위해\n",
      "Episode: 7/7 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 8 / 8\n",
      "\n",
      "setPoint (특정 y값): 245 을 맞추기위해 위해\n",
      "Episode: 8/58 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 9 / 9\n",
      "\n",
      "setPoint (특정 y값): 292 을 맞추기위해 위해\n",
      "Episode: 9/63 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 10 / 10\n",
      "\n",
      "setPoint (특정 y값): 324 을 맞추기위해 위해\n",
      "Episode: 10/53 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 11 / 11\n",
      "\n",
      "setPoint (특정 y값): 277 을 맞추기위해 위해\n",
      "Episode: 11/18 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 12 / 12\n",
      "\n",
      "setPoint (특정 y값): 215 을 맞추기위해 위해\n",
      "Episode: 12/40 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 13 / 13\n",
      "\n",
      "setPoint (특정 y값): 113 을 맞추기위해 위해\n",
      "Episode: 13/15 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 14 / 14\n",
      "\n",
      "setPoint (특정 y값): -8 을 맞추기위해 위해\n",
      "Episode: 14/41 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 15 / 15\n",
      "\n",
      "setPoint (특정 y값): 26 을 맞추기위해 위해\n",
      "Episode: 15/82 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 16 / 16\n",
      "\n",
      "setPoint (특정 y값): 151 을 맞추기위해 위해\n",
      "Episode: 16/31 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 17 / 17\n",
      "\n",
      "setPoint (특정 y값): 13 을 맞추기위해 위해\n",
      "Episode: 17/59 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 18 / 18\n",
      "\n",
      "setPoint (특정 y값): 168 을 맞추기위해 위해\n",
      "Episode: 18/47 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 19 / 19\n",
      "\n",
      "setPoint (특정 y값): 315 을 맞추기위해 위해\n",
      "Episode: 19/25 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 20 / 20\n",
      "\n",
      "setPoint (특정 y값): 353 을 맞추기위해 위해\n",
      "Episode: 20/86 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 21 / 21\n",
      "\n",
      "setPoint (특정 y값): 160 을 맞추기위해 위해\n",
      "Episode: 21/33 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 22 / 22\n",
      "\n",
      "setPoint (특정 y값): 42 을 맞추기위해 위해\n",
      "Episode: 22/57 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 23 / 23\n",
      "\n",
      "setPoint (특정 y값): 19 을 맞추기위해 위해\n",
      "Episode: 23/64 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 24 / 24\n",
      "\n",
      "setPoint (특정 y값): 173 을 맞추기위해 위해\n",
      "Episode: 24/20 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 25 / 25\n",
      "\n",
      "setPoint (특정 y값): 157 을 맞추기위해 위해\n",
      "Episode: 25/39 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 26 / 26\n",
      "\n",
      "setPoint (특정 y값): 279 을 맞추기위해 위해\n",
      "Episode: 26/62 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 27 / 27\n",
      "\n",
      "setPoint (특정 y값): 75 을 맞추기위해 위해\n",
      "Episode: 27/51 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 28 / 28\n",
      "\n",
      "setPoint (특정 y값): 82 을 맞추기위해 위해\n",
      "Episode: 28/13 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 29 / 29\n",
      "\n",
      "setPoint (특정 y값): -10 을 맞추기위해 위해\n",
      "Episode: 29/50 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 30 / 30\n",
      "\n",
      "setPoint (특정 y값): 96 을 맞추기위해 위해\n",
      "Episode: 30/3 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 31 / 31\n",
      "\n",
      "setPoint (특정 y값): 173 을 맞추기위해 위해\n",
      "Episode: 31/47 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 32 / 32\n",
      "\n",
      "setPoint (특정 y값): 284 을 맞추기위해 위해\n",
      "Episode: 32/9 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 33 / 33\n",
      "\n",
      "setPoint (특정 y값): 0 을 맞추기위해 위해\n",
      "Episode: 33/40 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 34 / 34\n",
      "\n",
      "setPoint (특정 y값): 37 을 맞추기위해 위해\n",
      "Episode: 34/62 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 35 / 35\n",
      "\n",
      "setPoint (특정 y값): 308 을 맞추기위해 위해\n",
      "Episode: 35/51 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 36 / 36\n",
      "\n",
      "setPoint (특정 y값): 255 을 맞추기위해 위해\n",
      "Episode: 36/28 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 37 / 37\n",
      "\n",
      "setPoint (특정 y값): 46 을 맞추기위해 위해\n",
      "Episode: 37/26 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 38 / 38\n",
      "\n",
      "setPoint (특정 y값): 341 을 맞추기위해 위해\n",
      "Episode: 38/81 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 39 / 39\n",
      "\n",
      "setPoint (특정 y값): 186 을 맞추기위해 위해\n",
      "Episode: 39/12 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 40 / 40\n",
      "\n",
      "setPoint (특정 y값): -8 을 맞추기위해 위해\n",
      "Episode: 40/24 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 41 / 41\n",
      "\n",
      "setPoint (특정 y값): 204 을 맞추기위해 위해\n",
      "Episode: 41/15 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 42 / 42\n",
      "\n",
      "setPoint (특정 y값): 171 을 맞추기위해 위해\n",
      "Episode: 42/44 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 43 / 43\n",
      "\n",
      "setPoint (특정 y값): 24 을 맞추기위해 위해\n",
      "Episode: 43/7 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 44 / 44\n",
      "\n",
      "setPoint (특정 y값): 252 을 맞추기위해 위해\n",
      "Episode: 44/78 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 45 / 45\n",
      "\n",
      "setPoint (특정 y값): 23 을 맞추기위해 위해\n",
      "Episode: 45/96 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 46 / 46\n",
      "\n",
      "setPoint (특정 y값): 276 을 맞추기위해 위해\n",
      "Episode: 46/61 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 47 / 47\n",
      "\n",
      "setPoint (특정 y값): 74 을 맞추기위해 위해\n",
      "Episode: 47/15 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 48 / 48\n",
      "\n",
      "setPoint (특정 y값): 91 을 맞추기위해 위해\n",
      "Episode: 48/14 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 49 / 49\n",
      "\n",
      "setPoint (특정 y값): 183 을 맞추기위해 위해\n",
      "Episode: 49/43 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 50 / 50\n",
      "\n",
      "setPoint (특정 y값): 281 을 맞추기위해 위해\n",
      "Episode: 50/91 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 51 / 51\n",
      "\n",
      "setPoint (특정 y값): 76 을 맞추기위해 위해\n",
      "Episode: 51/46 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 52 / 52\n",
      "\n",
      "setPoint (특정 y값): 64 을 맞추기위해 위해\n",
      "Episode: 52/36 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 53 / 53\n",
      "\n",
      "setPoint (특정 y값): 108 을 맞추기위해 위해\n",
      "Episode: 53/31 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 54 / 54\n",
      "\n",
      "setPoint (특정 y값): 164 을 맞추기위해 위해\n",
      "Episode: 54/21 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 55 / 55\n",
      "\n",
      "setPoint (특정 y값): 51 을 맞추기위해 위해\n",
      "Episode: 55/28 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 56 / 56\n",
      "\n",
      "setPoint (특정 y값): 343 을 맞추기위해 위해\n",
      "Episode: 56/55 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 57 / 57\n",
      "\n",
      "setPoint (특정 y값): 288 을 맞추기위해 위해\n",
      "Episode: 57/26 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 58 / 58\n",
      "\n",
      "setPoint (특정 y값): 316 을 맞추기위해 위해\n",
      "Episode: 58/51 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 59 / 59\n",
      "\n",
      "setPoint (특정 y값): 148 을 맞추기위해 위해\n",
      "Episode: 59/0 - Action 이후 x값과 예측 y값: (38, 143.89) / 수행 action: Down / 다음상태 y 값: tensor([4.1052]) / reward: 1.22\n",
      "loss: None\n",
      "성공! 1 / 60\n",
      "\n",
      "setPoint (특정 y값): 329 을 맞추기위해 위해\n",
      "Episode: 60/23 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 60 / 61\n",
      "\n",
      "setPoint (특정 y값): 188 을 맞추기위해 위해\n",
      "Episode: 61/11 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 61 / 62\n",
      "\n",
      "setPoint (특정 y값): 48 을 맞추기위해 위해\n",
      "Episode: 62/2 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 62 / 63\n",
      "\n",
      "setPoint (특정 y값): 220 을 맞추기위해 위해\n",
      "Episode: 63/5 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 63 / 64\n",
      "\n",
      "setPoint (특정 y값): -9 을 맞추기위해 위해\n",
      "Episode: 64/82 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 64 / 65\n",
      "\n",
      "setPoint (특정 y값): 94 을 맞추기위해 위해\n",
      "Episode: 65/36 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 65 / 66\n",
      "\n",
      "setPoint (특정 y값): 261 을 맞추기위해 위해\n",
      "Episode: 66/61 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 66 / 67\n",
      "\n",
      "setPoint (특정 y값): 103 을 맞추기위해 위해\n",
      "Episode: 67/12 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 67 / 68\n",
      "\n",
      "setPoint (특정 y값): 167 을 맞추기위해 위해\n",
      "Episode: 68/52 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 68 / 69\n",
      "\n",
      "setPoint (특정 y값): 6 을 맞추기위해 위해\n",
      "Episode: 69/40 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 69 / 70\n",
      "\n",
      "setPoint (특정 y값): 170 을 맞추기위해 위해\n",
      "Episode: 70/49 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 70 / 71\n",
      "\n",
      "setPoint (특정 y값): 164 을 맞추기위해 위해\n",
      "Episode: 71/0 - Action 이후 x값과 예측 y값: (45, 168.4) / 수행 action: Up / 다음상태 y 값: tensor([-4.4009]) / reward: 1.14\n",
      "loss: None\n",
      "성공! 2 / 72\n",
      "\n",
      "setPoint (특정 y값): 169 을 맞추기위해 위해\n",
      "Episode: 72/0 - Action 이후 x값과 예측 y값: (44, 164.9) / 수행 action: Down / 다음상태 y 값: tensor([4.1000]) / reward: 1.22\n",
      "loss: None\n",
      "성공! 3 / 73\n",
      "\n",
      "setPoint (특정 y값): 6 을 맞추기위해 위해\n",
      "Episode: 73/7 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 71 / 74\n",
      "\n",
      "setPoint (특정 y값): 262 을 맞추기위해 위해\n",
      "Episode: 74/10 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 72 / 75\n",
      "\n",
      "setPoint (특정 y값): 204 을 맞추기위해 위해\n",
      "Episode: 75/15 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 73 / 76\n",
      "\n",
      "setPoint (특정 y값): 310 을 맞추기위해 위해\n",
      "Episode: 76/32 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 74 / 77\n",
      "\n",
      "setPoint (특정 y값): 232 을 맞추기위해 위해\n",
      "Episode: 77/11 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 75 / 78\n",
      "\n",
      "setPoint (특정 y값): 143 을 맞추기위해 위해\n",
      "Episode: 78/0 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 76 / 79\n",
      "\n",
      "setPoint (특정 y값): 43 을 맞추기위해 위해\n",
      "Episode: 79/28 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 77 / 80\n",
      "\n",
      "setPoint (특정 y값): 210 을 맞추기위해 위해\n",
      "Episode: 80/0 - Action 이후 x값과 예측 y값: (57, 210.41) / 수행 action: Up / 다음상태 y 값: tensor([-0.4113]) / reward: 12.16\n",
      "loss: None\n",
      "성공! 4 / 81\n",
      "\n",
      "setPoint (특정 y값): 344 을 맞추기위해 위해\n",
      "Episode: 81/61 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 78 / 82\n",
      "\n",
      "setPoint (특정 y값): 292 을 맞추기위해 위해\n",
      "Episode: 82/57 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 79 / 83\n",
      "\n",
      "setPoint (특정 y값): 347 을 맞추기위해 위해\n",
      "Episode: 83/19 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 80 / 84\n",
      "\n",
      "setPoint (특정 y값): 292 을 맞추기위해 위해\n",
      "Episode: 84/51 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 81 / 85\n",
      "\n",
      "setPoint (특정 y값): 10 을 맞추기위해 위해\n",
      "Episode: 85/55 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 82 / 86\n",
      "\n",
      "setPoint (특정 y값): 33 을 맞추기위해 위해\n",
      "Episode: 86/5 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 83 / 87\n",
      "\n",
      "setPoint (특정 y값): 176 을 맞추기위해 위해\n",
      "Episode: 87/42 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 84 / 88\n",
      "\n",
      "setPoint (특정 y값): 269 을 맞추기위해 위해\n",
      "Episode: 88/37 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 85 / 89\n",
      "\n",
      "setPoint (특정 y값): 250 을 맞추기위해 위해\n",
      "Episode: 89/2 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 86 / 90\n",
      "\n",
      "setPoint (특정 y값): 291 을 맞추기위해 위해\n",
      "Episode: 90/78 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 87 / 91\n",
      "\n",
      "setPoint (특정 y값): 142 을 맞추기위해 위해\n",
      "Episode: 91/44 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 88 / 92\n",
      "\n",
      "setPoint (특정 y값): 343 을 맞추기위해 위해\n",
      "Episode: 92/110 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 89 / 93\n",
      "\n",
      "setPoint (특정 y값): -11 을 맞추기위해 위해\n",
      "Episode: 93/70 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 90 / 94\n",
      "\n",
      "setPoint (특정 y값): 173 을 맞추기위해 위해\n",
      "Episode: 94/0 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 91 / 95\n",
      "\n",
      "setPoint (특정 y값): 3 을 맞추기위해 위해\n",
      "Episode: 95/83 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 92 / 96\n",
      "\n",
      "setPoint (특정 y값): 332 을 맞추기위해 위해\n",
      "Episode: 96/89 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 93 / 97\n",
      "\n",
      "setPoint (특정 y값): 227 을 맞추기위해 위해\n",
      "Episode: 97/2 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 94 / 98\n",
      "\n",
      "setPoint (특정 y값): 55 을 맞추기위해 위해\n",
      "Episode: 98/0 - Action 이후 x값과 예측 y값: (-11, -24.15) / 수행 action: Down / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 95 / 99\n",
      "\n",
      "setPoint (특정 y값): 12 을 맞추기위해 위해\n",
      "Episode: 99/68 - Action 이후 x값과 예측 y값: (101, 360.95) / 수행 action: Up / 다음상태 y 값: None / reward: -100.0\n",
      "loss: None\n",
      "실패! 96 / 100\n",
      "\n",
      "Complete 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "success_cnt = 0\n",
    "fail_cnt = 0\n",
    "num_episodes = 100\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    epMemory = list()\n",
    "    z = random.randrange(-10, 100)\n",
    "    setPoint = random.randrange(math.floor(min(data['y'])), math.ceil(max(data['y'])))\n",
    "    env = Environment(z=z, setPoint=setPoint)\n",
    "    state = env.reset()\n",
    "    print(\"setPoint (특정 y값):\", setPoint, \"을 맞추기위해 위해\")\n",
    "    for t in count():\n",
    "        # 행동 결정\n",
    "        action = select_action(torch.tensor([state]).float())\n",
    "        next_state, reward, done = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        \n",
    "        # 메모리에 경험 저장\n",
    "        epMemory.append([state, action, next_state, reward])\n",
    "        \n",
    "        # 다음 상태로 이동\n",
    "        state = next_state\n",
    "        \n",
    "        loss = optimize_model()\n",
    "        \n",
    "        msg = f\"Episode: {i_episode}/{t} - Action 이후 x값과 예측 y값: {env.render()} / 수행 action: {Action(action)} / 다음상태 y 값: {next_state} / reward: {round(reward.item(),2)}\"\n",
    "        # print(msg)\n",
    "        # print('loss:',loss)\n",
    "        \n",
    "        if done:\n",
    "            print(msg)\n",
    "            print('loss:',loss)\n",
    "\n",
    "            cur_x,pred = env.render()\n",
    "            if -10 <= cur_x <= 100:\n",
    "                success_cnt+=1\n",
    "                _ = [memory.push(epMemory[i][0], epMemory[i][1], epMemory[i][2], epMemory[i][3]) for i in range(len(epMemory))]\n",
    "                print(f\"성공! {success_cnt} / {success_cnt+fail_cnt}\")\n",
    "                \n",
    "            else:\n",
    "                fail_cnt += 1\n",
    "                print(f\"실패! {fail_cnt} / {success_cnt+fail_cnt}\")\n",
    "            print(\"\")\n",
    "            break\n",
    "\n",
    "        if t >= 10000:\n",
    "            print(\"중단!\")\n",
    "            break\n",
    "        \n",
    "    # 주기적 네트워크 업데이트, 타겟 네트워크 업데이트  \n",
    "    if i_episode % TARGET_UPDATE == 0:  # 10번에 한버씩 \n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print(f\"Complete {success_cnt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "episode를 수행하기 위해 현재 x(코드에서는 z)값과 setPoint 모두 random하게 생성하고, 현 state에서 reward를 최대화하기 위해 가장 좋은 다음 action을 취하는 방법을 학습한다. \n",
    "\n",
    "성공한 episode만 memory에 저장하여 학습하였다. (Pytorch Tutorial에서는 모든 episode를 다 저장한다.)\n",
    "\n",
    "또한, 한 episode에서 step이 10,000번이 넘어가면 중단하고 다음 episode를 새롭게 시작하도록 구현하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 검증 및 Simulation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 이 학습된 모델을 실제 환경에 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "목표 품질 y값: 342\n"
     ]
    }
   ],
   "source": [
    "\n",
    "setPoint = random.randrange(math.floor(min(data['y'])), math.ceil(max(data['y'])))\n",
    "\n",
    "print('목표 품질 y값:', setPoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setPoint 342 을 맞추기 위해\n",
      "action tensor([0], device='cuda:0')\n",
      "0 - Action 이후 x값과 예측 y값: (4, 24.87)에서 Down 하면 setPoint(y) 값은 tensor([317.1347]) / reward: 0.02\n",
      "action tensor([1], device='cuda:0')\n",
      "1 - Action 이후 x값과 예측 y값: (5, 28.37)에서 Up 하면 setPoint(y) 값은 tensor([313.6339]) / reward: 0.02\n",
      "action tensor([0], device='cuda:0')\n",
      "2 - Action 이후 x값과 예측 y값: (4, 24.87)에서 Down 하면 setPoint(y) 값은 tensor([317.1347]) / reward: 0.02\n",
      "action tensor([0], device='cuda:0')\n",
      "3 - Action 이후 x값과 예측 y값: (3, 21.36)에서 Down 하면 setPoint(y) 값은 tensor([320.6356]) / reward: 0.02\n",
      "action tensor([0], device='cuda:0')\n",
      "4 - Action 이후 x값과 예측 y값: (2, 17.86)에서 Down 하면 setPoint(y) 값은 tensor([324.1364]) / reward: 0.02\n",
      "action tensor([0], device='cuda:0')\n",
      "5 - Action 이후 x값과 예측 y값: (1, 14.36)에서 Down 하면 setPoint(y) 값은 tensor([327.6373]) / reward: 0.02\n",
      "action tensor([0], device='cuda:0')\n",
      "6 - Action 이후 x값과 예측 y값: (0, 10.86)에서 Down 하면 setPoint(y) 값은 tensor([331.1382]) / reward: 0.02\n",
      "action tensor([0], device='cuda:0')\n",
      "7 - Action 이후 x값과 예측 y값: (-1, 7.36)에서 Down 하면 setPoint(y) 값은 tensor([334.6390]) / reward: 0.01\n",
      "action tensor([0], device='cuda:0')\n",
      "8 - Action 이후 x값과 예측 y값: (-2, 3.86)에서 Down 하면 setPoint(y) 값은 tensor([338.1399]) / reward: 0.01\n",
      "action tensor([0], device='cuda:0')\n",
      "9 - Action 이후 x값과 예측 y값: (-3, 0.36)에서 Down 하면 setPoint(y) 값은 tensor([341.6408]) / reward: 0.01\n",
      "action tensor([0], device='cuda:0')\n",
      "10 - Action 이후 x값과 예측 y값: (-4, -3.14)에서 Down 하면 setPoint(y) 값은 tensor([345.1417]) / reward: 0.01\n",
      "action tensor([0], device='cuda:0')\n",
      "11 - Action 이후 x값과 예측 y값: (-5, -6.64)에서 Down 하면 setPoint(y) 값은 tensor([348.6425]) / reward: 0.01\n",
      "action tensor([0], device='cuda:0')\n",
      "12 - Action 이후 x값과 예측 y값: (-6, -10.14)에서 Down 하면 setPoint(y) 값은 tensor([352.1434]) / reward: 0.01\n",
      "action tensor([0], device='cuda:0')\n",
      "13 - Action 이후 x값과 예측 y값: (-7, -13.64)에서 Down 하면 setPoint(y) 값은 tensor([355.6443]) / reward: 0.01\n",
      "action tensor([0], device='cuda:0')\n",
      "14 - Action 이후 x값과 예측 y값: (-8, -17.15)에서 Down 하면 setPoint(y) 값은 tensor([359.1451]) / reward: 0.01\n",
      "action tensor([0], device='cuda:0')\n",
      "15 - Action 이후 x값과 예측 y값: (-9, -20.65)에서 Down 하면 setPoint(y) 값은 tensor([362.6460]) / reward: 0.01\n",
      "action tensor([0], device='cuda:0')\n",
      "16 - Action 이후 x값과 예측 y값: (-10, -24.15)에서 Down 하면 setPoint(y) 값은 tensor([366.1469]) / reward: 0.01\n",
      "action tensor([0], device='cuda:0')\n",
      "17 - Action 이후 x값과 예측 y값: (-11, -24.15)에서 Down 하면 setPoint(y) 값은 None / reward: -100.0\n",
      "실패!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = random.randrange(-10, 100)\n",
    "env = Environment(z=z, setPoint=setPoint)\n",
    "\n",
    "yPred = model.predict(np.array([z]).reshape(-1,1)).item()\n",
    "state = torch.tensor([setPoint - yPred])    # 지금 y 값\n",
    "# 이 state에서 setPoint가 되기 위해서 어떤 action을 취하라 \n",
    "\n",
    "print(\"setPoint\", setPoint, \"을 맞추기 위해\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for t in count():\n",
    "        action = select_action(torch.tensor([state]).float())\n",
    "        print('action',action)\n",
    "        next_state, reward, done = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "        msg = f\"{t} - Action 이후 x값과 예측 y값: {env.render()}에서 {Action(action)} 하면 setPoint(y) 값은 {next_state} / reward: {round(reward.item(),2)}\"\n",
    "        print(msg)\n",
    "\n",
    "        # 다음 상태로 이동 \n",
    "        state = next_state \n",
    "        \n",
    "        if done:\n",
    "            cur_x,pred = env.render()\n",
    "            if -10 <= cur_x <= 100:\n",
    "                print(f\"성공! 입력:{cur_x}, 출력:{pred}, 목표:{setPoint}\")\n",
    "            else:\n",
    "                print(\"실패!\")\n",
    "            print()\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "목표 품질 y값: 37\n"
     ]
    }
   ],
   "source": [
    "\n",
    "setPoint = random.randrange(math.floor(min(data['y'])), math.ceil(max(data['y'])))\n",
    "\n",
    "print('목표 품질 y값:', setPoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0번째 시도 setPoint 37를 맞추기 위해\n",
      "0/10-0 - Action1 이후 x값과 예측 y값: (13, 56.37)에서 Up 하면 setPoint(y) 값은 tensor([-19.3731]) / reward: 0.26\n",
      "0/10-0 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-1 - Action1 이후 x값과 예측 y값: (14, 59.87)에서 Up 하면 setPoint(y) 값은 tensor([-22.8740]) / reward: 0.22\n",
      "0/10-1 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-2 - Action1 이후 x값과 예측 y값: (15, 63.37)에서 Up 하면 setPoint(y) 값은 tensor([-26.3748]) / reward: 0.19\n",
      "0/10-2 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-3 - Action1 이후 x값과 예측 y값: (16, 66.88)에서 Up 하면 setPoint(y) 값은 tensor([-29.8757]) / reward: 0.17\n",
      "0/10-3 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-4 - Action1 이후 x값과 예측 y값: (17, 70.38)에서 Up 하면 setPoint(y) 값은 tensor([-33.3766]) / reward: 0.15\n",
      "0/10-4 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-5 - Action1 이후 x값과 예측 y값: (18, 73.88)에서 Up 하면 setPoint(y) 값은 tensor([-36.8774]) / reward: 0.14\n",
      "0/10-5 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-6 - Action1 이후 x값과 예측 y값: (19, 77.38)에서 Up 하면 setPoint(y) 값은 tensor([-40.3783]) / reward: 0.12\n",
      "0/10-6 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-7 - Action1 이후 x값과 예측 y값: (20, 80.88)에서 Up 하면 setPoint(y) 값은 tensor([-43.8792]) / reward: 0.11\n",
      "0/10-7 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-8 - Action1 이후 x값과 예측 y값: (19, 77.38)에서 Down 하면 setPoint(y) 값은 tensor([-40.3783]) / reward: 0.12\n",
      "0/10-8 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-9 - Action1 이후 x값과 예측 y값: (20, 80.88)에서 Up 하면 setPoint(y) 값은 tensor([-43.8792]) / reward: 0.11\n",
      "0/10-9 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-10 - Action1 이후 x값과 예측 y값: (21, 84.38)에서 Up 하면 setPoint(y) 값은 tensor([-47.3800]) / reward: 0.11\n",
      "0/10-10 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-11 - Action1 이후 x값과 예측 y값: (22, 87.88)에서 Up 하면 setPoint(y) 값은 tensor([-50.8809]) / reward: 0.1\n",
      "0/10-11 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-12 - Action1 이후 x값과 예측 y값: (23, 91.38)에서 Up 하면 setPoint(y) 값은 tensor([-54.3818]) / reward: 0.09\n",
      "0/10-12 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-13 - Action1 이후 x값과 예측 y값: (24, 94.88)에서 Up 하면 setPoint(y) 값은 tensor([-57.8826]) / reward: 0.09\n",
      "0/10-13 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-14 - Action1 이후 x값과 예측 y값: (25, 98.38)에서 Up 하면 setPoint(y) 값은 tensor([-61.3835]) / reward: 0.08\n",
      "0/10-14 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-15 - Action1 이후 x값과 예측 y값: (26, 101.88)에서 Up 하면 setPoint(y) 값은 tensor([-64.8844]) / reward: 0.08\n",
      "0/10-15 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-16 - Action1 이후 x값과 예측 y값: (27, 105.39)에서 Up 하면 setPoint(y) 값은 tensor([-68.3853]) / reward: 0.07\n",
      "0/10-16 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-17 - Action1 이후 x값과 예측 y값: (28, 108.89)에서 Up 하면 setPoint(y) 값은 tensor([-71.8861]) / reward: 0.07\n",
      "0/10-17 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-18 - Action1 이후 x값과 예측 y값: (29, 112.39)에서 Up 하면 setPoint(y) 값은 tensor([-75.3870]) / reward: 0.07\n",
      "0/10-18 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-19 - Action1 이후 x값과 예측 y값: (30, 115.89)에서 Up 하면 setPoint(y) 값은 tensor([-78.8879]) / reward: 0.06\n",
      "0/10-19 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-20 - Action1 이후 x값과 예측 y값: (31, 119.39)에서 Up 하면 setPoint(y) 값은 tensor([-82.3887]) / reward: 0.06\n",
      "0/10-20 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-21 - Action1 이후 x값과 예측 y값: (32, 122.89)에서 Up 하면 setPoint(y) 값은 tensor([-85.8896]) / reward: 0.06\n",
      "0/10-21 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-22 - Action1 이후 x값과 예측 y값: (33, 126.39)에서 Up 하면 setPoint(y) 값은 tensor([-89.3905]) / reward: 0.06\n",
      "0/10-22 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-23 - Action1 이후 x값과 예측 y값: (34, 129.89)에서 Up 하면 setPoint(y) 값은 tensor([-92.8913]) / reward: 0.05\n",
      "0/10-23 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-24 - Action1 이후 x값과 예측 y값: (35, 133.39)에서 Up 하면 setPoint(y) 값은 tensor([-96.3922]) / reward: 0.05\n",
      "0/10-24 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-25 - Action1 이후 x값과 예측 y값: (36, 136.89)에서 Up 하면 setPoint(y) 값은 tensor([-99.8931]) / reward: 0.05\n",
      "0/10-25 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-26 - Action1 이후 x값과 예측 y값: (37, 140.39)에서 Up 하면 setPoint(y) 값은 tensor([-103.3939]) / reward: 0.05\n",
      "0/10-26 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-27 - Action1 이후 x값과 예측 y값: (38, 143.89)에서 Up 하면 setPoint(y) 값은 tensor([-106.8948]) / reward: 0.05\n",
      "0/10-27 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-28 - Action1 이후 x값과 예측 y값: (39, 147.4)에서 Up 하면 setPoint(y) 값은 tensor([-110.3957]) / reward: 0.05\n",
      "0/10-28 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-29 - Action1 이후 x값과 예측 y값: (40, 150.9)에서 Up 하면 setPoint(y) 값은 tensor([-113.8965]) / reward: 0.04\n",
      "0/10-29 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-30 - Action1 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "0/10-30 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-31 - Action1 이후 x값과 예측 y값: (42, 157.9)에서 Up 하면 setPoint(y) 값은 tensor([-120.8983]) / reward: 0.04\n",
      "0/10-31 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-32 - Action1 이후 x값과 예측 y값: (43, 161.4)에서 Up 하면 setPoint(y) 값은 tensor([-124.3991]) / reward: 0.04\n",
      "0/10-32 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-33 - Action1 이후 x값과 예측 y값: (44, 164.9)에서 Up 하면 setPoint(y) 값은 tensor([-127.9000]) / reward: 0.04\n",
      "0/10-33 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-34 - Action1 이후 x값과 예측 y값: (45, 168.4)에서 Up 하면 setPoint(y) 값은 tensor([-131.4009]) / reward: 0.04\n",
      "0/10-34 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-35 - Action1 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "0/10-35 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-36 - Action1 이후 x값과 예측 y값: (47, 175.4)에서 Up 하면 setPoint(y) 값은 tensor([-138.4026]) / reward: 0.04\n",
      "0/10-36 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-37 - Action1 이후 x값과 예측 y값: (48, 178.9)에서 Up 하면 setPoint(y) 값은 tensor([-141.9035]) / reward: 0.04\n",
      "0/10-37 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-38 - Action1 이후 x값과 예측 y값: (49, 182.4)에서 Up 하면 setPoint(y) 값은 tensor([-145.4044]) / reward: 0.03\n",
      "0/10-38 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-39 - Action1 이후 x값과 예측 y값: (50, 185.91)에서 Up 하면 setPoint(y) 값은 tensor([-148.9052]) / reward: 0.03\n",
      "0/10-39 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-40 - Action1 이후 x값과 예측 y값: (51, 189.41)에서 Up 하면 setPoint(y) 값은 tensor([-152.4061]) / reward: 0.03\n",
      "0/10-40 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-41 - Action1 이후 x값과 예측 y값: (52, 192.91)에서 Up 하면 setPoint(y) 값은 tensor([-155.9070]) / reward: 0.03\n",
      "0/10-41 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-42 - Action1 이후 x값과 예측 y값: (53, 196.41)에서 Up 하면 setPoint(y) 값은 tensor([-159.4078]) / reward: 0.03\n",
      "0/10-42 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-43 - Action1 이후 x값과 예측 y값: (54, 199.91)에서 Up 하면 setPoint(y) 값은 tensor([-162.9087]) / reward: 0.03\n",
      "0/10-43 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-44 - Action1 이후 x값과 예측 y값: (55, 203.41)에서 Up 하면 setPoint(y) 값은 tensor([-166.4096]) / reward: 0.03\n",
      "0/10-44 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-45 - Action1 이후 x값과 예측 y값: (56, 206.91)에서 Up 하면 setPoint(y) 값은 tensor([-169.9104]) / reward: 0.03\n",
      "0/10-45 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-46 - Action1 이후 x값과 예측 y값: (57, 210.41)에서 Up 하면 setPoint(y) 값은 tensor([-173.4113]) / reward: 0.03\n",
      "0/10-46 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-47 - Action1 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "0/10-47 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-48 - Action1 이후 x값과 예측 y값: (59, 217.41)에서 Up 하면 setPoint(y) 값은 tensor([-180.4130]) / reward: 0.03\n",
      "0/10-48 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-49 - Action1 이후 x값과 예측 y값: (60, 220.91)에서 Up 하면 setPoint(y) 값은 tensor([-183.9139]) / reward: 0.03\n",
      "0/10-49 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-50 - Action1 이후 x값과 예측 y값: (61, 224.41)에서 Up 하면 setPoint(y) 값은 tensor([-187.4148]) / reward: 0.03\n",
      "0/10-50 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-51 - Action1 이후 x값과 예측 y값: (62, 227.92)에서 Up 하면 setPoint(y) 값은 tensor([-190.9156]) / reward: 0.03\n",
      "0/10-51 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-52 - Action1 이후 x값과 예측 y값: (63, 231.42)에서 Up 하면 setPoint(y) 값은 tensor([-194.4165]) / reward: 0.03\n",
      "0/10-52 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-53 - Action1 이후 x값과 예측 y값: (64, 234.92)에서 Up 하면 setPoint(y) 값은 tensor([-197.9174]) / reward: 0.03\n",
      "0/10-53 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-54 - Action1 이후 x값과 예측 y값: (65, 238.42)에서 Up 하면 setPoint(y) 값은 tensor([-201.4182]) / reward: 0.02\n",
      "0/10-54 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-55 - Action1 이후 x값과 예측 y값: (66, 241.92)에서 Up 하면 setPoint(y) 값은 tensor([-204.9191]) / reward: 0.02\n",
      "0/10-55 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-56 - Action1 이후 x값과 예측 y값: (67, 245.42)에서 Up 하면 setPoint(y) 값은 tensor([-208.4200]) / reward: 0.02\n",
      "0/10-56 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-57 - Action1 이후 x값과 예측 y값: (68, 248.92)에서 Up 하면 setPoint(y) 값은 tensor([-211.9209]) / reward: 0.02\n",
      "0/10-57 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-58 - Action1 이후 x값과 예측 y값: (69, 252.42)에서 Up 하면 setPoint(y) 값은 tensor([-215.4217]) / reward: 0.02\n",
      "0/10-58 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-59 - Action1 이후 x값과 예측 y값: (68, 248.92)에서 Down 하면 setPoint(y) 값은 tensor([-211.9209]) / reward: 0.02\n",
      "0/10-59 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-60 - Action1 이후 x값과 예측 y값: (69, 252.42)에서 Up 하면 setPoint(y) 값은 tensor([-215.4217]) / reward: 0.02\n",
      "0/10-60 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-61 - Action1 이후 x값과 예측 y값: (68, 248.92)에서 Down 하면 setPoint(y) 값은 tensor([-211.9209]) / reward: 0.02\n",
      "0/10-61 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-62 - Action1 이후 x값과 예측 y값: (69, 252.42)에서 Up 하면 setPoint(y) 값은 tensor([-215.4217]) / reward: 0.02\n",
      "0/10-62 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-63 - Action1 이후 x값과 예측 y값: (70, 255.92)에서 Up 하면 setPoint(y) 값은 tensor([-218.9226]) / reward: 0.02\n",
      "0/10-63 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-64 - Action1 이후 x값과 예측 y값: (71, 259.42)에서 Up 하면 setPoint(y) 값은 tensor([-222.4234]) / reward: 0.02\n",
      "0/10-64 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-65 - Action1 이후 x값과 예측 y값: (72, 262.92)에서 Up 하면 setPoint(y) 값은 tensor([-225.9243]) / reward: 0.02\n",
      "0/10-65 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-66 - Action1 이후 x값과 예측 y값: (73, 266.43)에서 Up 하면 setPoint(y) 값은 tensor([-229.4252]) / reward: 0.02\n",
      "0/10-66 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-67 - Action1 이후 x값과 예측 y값: (74, 269.93)에서 Up 하면 setPoint(y) 값은 tensor([-232.9261]) / reward: 0.02\n",
      "0/10-67 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-68 - Action1 이후 x값과 예측 y값: (75, 273.43)에서 Up 하면 setPoint(y) 값은 tensor([-236.4269]) / reward: 0.02\n",
      "0/10-68 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-69 - Action1 이후 x값과 예측 y값: (76, 276.93)에서 Up 하면 setPoint(y) 값은 tensor([-239.9278]) / reward: 0.02\n",
      "0/10-69 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-70 - Action1 이후 x값과 예측 y값: (77, 280.43)에서 Up 하면 setPoint(y) 값은 tensor([-243.4287]) / reward: 0.02\n",
      "0/10-70 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-71 - Action1 이후 x값과 예측 y값: (78, 283.93)에서 Up 하면 setPoint(y) 값은 tensor([-246.9295]) / reward: 0.02\n",
      "0/10-71 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-72 - Action1 이후 x값과 예측 y값: (79, 287.43)에서 Up 하면 setPoint(y) 값은 tensor([-250.4304]) / reward: 0.02\n",
      "0/10-72 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-73 - Action1 이후 x값과 예측 y값: (80, 290.93)에서 Up 하면 setPoint(y) 값은 tensor([-253.9313]) / reward: 0.02\n",
      "0/10-73 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-74 - Action1 이후 x값과 예측 y값: (81, 294.43)에서 Up 하면 setPoint(y) 값은 tensor([-257.4321]) / reward: 0.02\n",
      "0/10-74 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-75 - Action1 이후 x값과 예측 y값: (82, 297.93)에서 Up 하면 setPoint(y) 값은 tensor([-260.9330]) / reward: 0.02\n",
      "0/10-75 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-76 - Action1 이후 x값과 예측 y값: (83, 301.43)에서 Up 하면 setPoint(y) 값은 tensor([-264.4339]) / reward: 0.02\n",
      "0/10-76 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-77 - Action1 이후 x값과 예측 y값: (84, 304.93)에서 Up 하면 setPoint(y) 값은 tensor([-267.9347]) / reward: 0.02\n",
      "0/10-77 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-78 - Action1 이후 x값과 예측 y값: (85, 308.44)에서 Up 하면 setPoint(y) 값은 tensor([-271.4356]) / reward: 0.02\n",
      "0/10-78 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-79 - Action1 이후 x값과 예측 y값: (86, 311.94)에서 Up 하면 setPoint(y) 값은 tensor([-274.9365]) / reward: 0.02\n",
      "0/10-79 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-80 - Action1 이후 x값과 예측 y값: (87, 315.44)에서 Up 하면 setPoint(y) 값은 tensor([-278.4373]) / reward: 0.02\n",
      "0/10-80 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-81 - Action1 이후 x값과 예측 y값: (88, 318.94)에서 Up 하면 setPoint(y) 값은 tensor([-281.9382]) / reward: 0.02\n",
      "0/10-81 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-82 - Action1 이후 x값과 예측 y값: (89, 322.44)에서 Up 하면 setPoint(y) 값은 tensor([-285.4391]) / reward: 0.02\n",
      "0/10-82 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-83 - Action1 이후 x값과 예측 y값: (90, 325.94)에서 Up 하면 setPoint(y) 값은 tensor([-288.9399]) / reward: 0.02\n",
      "0/10-83 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-84 - Action1 이후 x값과 예측 y값: (91, 329.44)에서 Up 하면 setPoint(y) 값은 tensor([-292.4408]) / reward: 0.02\n",
      "0/10-84 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-85 - Action1 이후 x값과 예측 y값: (92, 332.94)에서 Up 하면 setPoint(y) 값은 tensor([-295.9417]) / reward: 0.02\n",
      "0/10-85 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-86 - Action1 이후 x값과 예측 y값: (93, 336.44)에서 Up 하면 setPoint(y) 값은 tensor([-299.4425]) / reward: 0.02\n",
      "0/10-86 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-87 - Action1 이후 x값과 예측 y값: (94, 339.94)에서 Up 하면 setPoint(y) 값은 tensor([-302.9434]) / reward: 0.02\n",
      "0/10-87 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-88 - Action1 이후 x값과 예측 y값: (95, 343.44)에서 Up 하면 setPoint(y) 값은 tensor([-306.4443]) / reward: 0.02\n",
      "0/10-88 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-89 - Action1 이후 x값과 예측 y값: (94, 339.94)에서 Down 하면 setPoint(y) 값은 tensor([-302.9434]) / reward: 0.02\n",
      "0/10-89 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-90 - Action1 이후 x값과 예측 y값: (95, 343.44)에서 Up 하면 setPoint(y) 값은 tensor([-306.4443]) / reward: 0.02\n",
      "0/10-90 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-91 - Action1 이후 x값과 예측 y값: (96, 346.95)에서 Up 하면 setPoint(y) 값은 tensor([-309.9452]) / reward: 0.02\n",
      "0/10-91 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-92 - Action1 이후 x값과 예측 y값: (97, 350.45)에서 Up 하면 setPoint(y) 값은 tensor([-313.4460]) / reward: 0.02\n",
      "0/10-92 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-93 - Action1 이후 x값과 예측 y값: (98, 353.95)에서 Up 하면 setPoint(y) 값은 tensor([-316.9469]) / reward: 0.02\n",
      "0/10-93 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-94 - Action1 이후 x값과 예측 y값: (99, 357.45)에서 Up 하면 setPoint(y) 값은 tensor([-320.4478]) / reward: 0.02\n",
      "0/10-94 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-95 - Action1 이후 x값과 예측 y값: (100, 360.95)에서 Up 하면 setPoint(y) 값은 tensor([-323.9486]) / reward: 0.02\n",
      "0/10-95 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "0/10-96 - Action1 이후 x값과 예측 y값: (101, 360.95)에서 Up 하면 setPoint(y) 값은 None / reward: -100.0\n",
      "0/10-96 - Action2 이후 x값과 예측 y값: (12, 52.87)에서 Up 하면 setPoint(y) 값은 tensor([-15.8722]) / reward: 0.32\n",
      "실패1!\n",
      "False False\n",
      "1번째 시도 setPoint 37를 맞추기 위해\n",
      "1/10-0 - Action1 이후 x값과 예측 y값: (42, 157.9)에서 Up 하면 setPoint(y) 값은 tensor([-120.8983]) / reward: 0.04\n",
      "1/10-0 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-1 - Action1 이후 x값과 예측 y값: (43, 161.4)에서 Up 하면 setPoint(y) 값은 tensor([-124.3991]) / reward: 0.04\n",
      "1/10-1 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-2 - Action1 이후 x값과 예측 y값: (44, 164.9)에서 Up 하면 setPoint(y) 값은 tensor([-127.9000]) / reward: 0.04\n",
      "1/10-2 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-3 - Action1 이후 x값과 예측 y값: (45, 168.4)에서 Up 하면 setPoint(y) 값은 tensor([-131.4009]) / reward: 0.04\n",
      "1/10-3 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-4 - Action1 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "1/10-4 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-5 - Action1 이후 x값과 예측 y값: (47, 175.4)에서 Up 하면 setPoint(y) 값은 tensor([-138.4026]) / reward: 0.04\n",
      "1/10-5 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-6 - Action1 이후 x값과 예측 y값: (48, 178.9)에서 Up 하면 setPoint(y) 값은 tensor([-141.9035]) / reward: 0.04\n",
      "1/10-6 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-7 - Action1 이후 x값과 예측 y값: (49, 182.4)에서 Up 하면 setPoint(y) 값은 tensor([-145.4044]) / reward: 0.03\n",
      "1/10-7 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-8 - Action1 이후 x값과 예측 y값: (50, 185.91)에서 Up 하면 setPoint(y) 값은 tensor([-148.9052]) / reward: 0.03\n",
      "1/10-8 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-9 - Action1 이후 x값과 예측 y값: (51, 189.41)에서 Up 하면 setPoint(y) 값은 tensor([-152.4061]) / reward: 0.03\n",
      "1/10-9 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-10 - Action1 이후 x값과 예측 y값: (52, 192.91)에서 Up 하면 setPoint(y) 값은 tensor([-155.9070]) / reward: 0.03\n",
      "1/10-10 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-11 - Action1 이후 x값과 예측 y값: (53, 196.41)에서 Up 하면 setPoint(y) 값은 tensor([-159.4078]) / reward: 0.03\n",
      "1/10-11 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-12 - Action1 이후 x값과 예측 y값: (54, 199.91)에서 Up 하면 setPoint(y) 값은 tensor([-162.9087]) / reward: 0.03\n",
      "1/10-12 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-13 - Action1 이후 x값과 예측 y값: (55, 203.41)에서 Up 하면 setPoint(y) 값은 tensor([-166.4096]) / reward: 0.03\n",
      "1/10-13 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-14 - Action1 이후 x값과 예측 y값: (56, 206.91)에서 Up 하면 setPoint(y) 값은 tensor([-169.9104]) / reward: 0.03\n",
      "1/10-14 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-15 - Action1 이후 x값과 예측 y값: (57, 210.41)에서 Up 하면 setPoint(y) 값은 tensor([-173.4113]) / reward: 0.03\n",
      "1/10-15 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-16 - Action1 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "1/10-16 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-17 - Action1 이후 x값과 예측 y값: (59, 217.41)에서 Up 하면 setPoint(y) 값은 tensor([-180.4130]) / reward: 0.03\n",
      "1/10-17 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-18 - Action1 이후 x값과 예측 y값: (60, 220.91)에서 Up 하면 setPoint(y) 값은 tensor([-183.9139]) / reward: 0.03\n",
      "1/10-18 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-19 - Action1 이후 x값과 예측 y값: (61, 224.41)에서 Up 하면 setPoint(y) 값은 tensor([-187.4148]) / reward: 0.03\n",
      "1/10-19 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-20 - Action1 이후 x값과 예측 y값: (62, 227.92)에서 Up 하면 setPoint(y) 값은 tensor([-190.9156]) / reward: 0.03\n",
      "1/10-20 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-21 - Action1 이후 x값과 예측 y값: (63, 231.42)에서 Up 하면 setPoint(y) 값은 tensor([-194.4165]) / reward: 0.03\n",
      "1/10-21 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-22 - Action1 이후 x값과 예측 y값: (64, 234.92)에서 Up 하면 setPoint(y) 값은 tensor([-197.9174]) / reward: 0.03\n",
      "1/10-22 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-23 - Action1 이후 x값과 예측 y값: (65, 238.42)에서 Up 하면 setPoint(y) 값은 tensor([-201.4182]) / reward: 0.02\n",
      "1/10-23 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-24 - Action1 이후 x값과 예측 y값: (66, 241.92)에서 Up 하면 setPoint(y) 값은 tensor([-204.9191]) / reward: 0.02\n",
      "1/10-24 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-25 - Action1 이후 x값과 예측 y값: (67, 245.42)에서 Up 하면 setPoint(y) 값은 tensor([-208.4200]) / reward: 0.02\n",
      "1/10-25 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-26 - Action1 이후 x값과 예측 y값: (68, 248.92)에서 Up 하면 setPoint(y) 값은 tensor([-211.9209]) / reward: 0.02\n",
      "1/10-26 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-27 - Action1 이후 x값과 예측 y값: (69, 252.42)에서 Up 하면 setPoint(y) 값은 tensor([-215.4217]) / reward: 0.02\n",
      "1/10-27 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-28 - Action1 이후 x값과 예측 y값: (70, 255.92)에서 Up 하면 setPoint(y) 값은 tensor([-218.9226]) / reward: 0.02\n",
      "1/10-28 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-29 - Action1 이후 x값과 예측 y값: (71, 259.42)에서 Up 하면 setPoint(y) 값은 tensor([-222.4234]) / reward: 0.02\n",
      "1/10-29 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-30 - Action1 이후 x값과 예측 y값: (72, 262.92)에서 Up 하면 setPoint(y) 값은 tensor([-225.9243]) / reward: 0.02\n",
      "1/10-30 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-31 - Action1 이후 x값과 예측 y값: (73, 266.43)에서 Up 하면 setPoint(y) 값은 tensor([-229.4252]) / reward: 0.02\n",
      "1/10-31 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-32 - Action1 이후 x값과 예측 y값: (74, 269.93)에서 Up 하면 setPoint(y) 값은 tensor([-232.9261]) / reward: 0.02\n",
      "1/10-32 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-33 - Action1 이후 x값과 예측 y값: (75, 273.43)에서 Up 하면 setPoint(y) 값은 tensor([-236.4269]) / reward: 0.02\n",
      "1/10-33 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-34 - Action1 이후 x값과 예측 y값: (76, 276.93)에서 Up 하면 setPoint(y) 값은 tensor([-239.9278]) / reward: 0.02\n",
      "1/10-34 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-35 - Action1 이후 x값과 예측 y값: (77, 280.43)에서 Up 하면 setPoint(y) 값은 tensor([-243.4287]) / reward: 0.02\n",
      "1/10-35 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-36 - Action1 이후 x값과 예측 y값: (78, 283.93)에서 Up 하면 setPoint(y) 값은 tensor([-246.9295]) / reward: 0.02\n",
      "1/10-36 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-37 - Action1 이후 x값과 예측 y값: (79, 287.43)에서 Up 하면 setPoint(y) 값은 tensor([-250.4304]) / reward: 0.02\n",
      "1/10-37 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-38 - Action1 이후 x값과 예측 y값: (80, 290.93)에서 Up 하면 setPoint(y) 값은 tensor([-253.9313]) / reward: 0.02\n",
      "1/10-38 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-39 - Action1 이후 x값과 예측 y값: (81, 294.43)에서 Up 하면 setPoint(y) 값은 tensor([-257.4321]) / reward: 0.02\n",
      "1/10-39 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-40 - Action1 이후 x값과 예측 y값: (82, 297.93)에서 Up 하면 setPoint(y) 값은 tensor([-260.9330]) / reward: 0.02\n",
      "1/10-40 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-41 - Action1 이후 x값과 예측 y값: (83, 301.43)에서 Up 하면 setPoint(y) 값은 tensor([-264.4339]) / reward: 0.02\n",
      "1/10-41 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-42 - Action1 이후 x값과 예측 y값: (84, 304.93)에서 Up 하면 setPoint(y) 값은 tensor([-267.9347]) / reward: 0.02\n",
      "1/10-42 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-43 - Action1 이후 x값과 예측 y값: (85, 308.44)에서 Up 하면 setPoint(y) 값은 tensor([-271.4356]) / reward: 0.02\n",
      "1/10-43 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-44 - Action1 이후 x값과 예측 y값: (86, 311.94)에서 Up 하면 setPoint(y) 값은 tensor([-274.9365]) / reward: 0.02\n",
      "1/10-44 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-45 - Action1 이후 x값과 예측 y값: (87, 315.44)에서 Up 하면 setPoint(y) 값은 tensor([-278.4373]) / reward: 0.02\n",
      "1/10-45 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-46 - Action1 이후 x값과 예측 y값: (88, 318.94)에서 Up 하면 setPoint(y) 값은 tensor([-281.9382]) / reward: 0.02\n",
      "1/10-46 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-47 - Action1 이후 x값과 예측 y값: (89, 322.44)에서 Up 하면 setPoint(y) 값은 tensor([-285.4391]) / reward: 0.02\n",
      "1/10-47 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-48 - Action1 이후 x값과 예측 y값: (90, 325.94)에서 Up 하면 setPoint(y) 값은 tensor([-288.9399]) / reward: 0.02\n",
      "1/10-48 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-49 - Action1 이후 x값과 예측 y값: (91, 329.44)에서 Up 하면 setPoint(y) 값은 tensor([-292.4408]) / reward: 0.02\n",
      "1/10-49 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-50 - Action1 이후 x값과 예측 y값: (92, 332.94)에서 Up 하면 setPoint(y) 값은 tensor([-295.9417]) / reward: 0.02\n",
      "1/10-50 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-51 - Action1 이후 x값과 예측 y값: (91, 329.44)에서 Down 하면 setPoint(y) 값은 tensor([-292.4408]) / reward: 0.02\n",
      "1/10-51 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-52 - Action1 이후 x값과 예측 y값: (92, 332.94)에서 Up 하면 setPoint(y) 값은 tensor([-295.9417]) / reward: 0.02\n",
      "1/10-52 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-53 - Action1 이후 x값과 예측 y값: (93, 336.44)에서 Up 하면 setPoint(y) 값은 tensor([-299.4425]) / reward: 0.02\n",
      "1/10-53 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-54 - Action1 이후 x값과 예측 y값: (94, 339.94)에서 Up 하면 setPoint(y) 값은 tensor([-302.9434]) / reward: 0.02\n",
      "1/10-54 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-55 - Action1 이후 x값과 예측 y값: (95, 343.44)에서 Up 하면 setPoint(y) 값은 tensor([-306.4443]) / reward: 0.02\n",
      "1/10-55 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-56 - Action1 이후 x값과 예측 y값: (96, 346.95)에서 Up 하면 setPoint(y) 값은 tensor([-309.9452]) / reward: 0.02\n",
      "1/10-56 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-57 - Action1 이후 x값과 예측 y값: (97, 350.45)에서 Up 하면 setPoint(y) 값은 tensor([-313.4460]) / reward: 0.02\n",
      "1/10-57 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-58 - Action1 이후 x값과 예측 y값: (98, 353.95)에서 Up 하면 setPoint(y) 값은 tensor([-316.9469]) / reward: 0.02\n",
      "1/10-58 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-59 - Action1 이후 x값과 예측 y값: (99, 357.45)에서 Up 하면 setPoint(y) 값은 tensor([-320.4478]) / reward: 0.02\n",
      "1/10-59 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-60 - Action1 이후 x값과 예측 y값: (100, 360.95)에서 Up 하면 setPoint(y) 값은 tensor([-323.9486]) / reward: 0.02\n",
      "1/10-60 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "1/10-61 - Action1 이후 x값과 예측 y값: (101, 360.95)에서 Up 하면 setPoint(y) 값은 None / reward: -100.0\n",
      "1/10-61 - Action2 이후 x값과 예측 y값: (41, 154.4)에서 Up 하면 setPoint(y) 값은 tensor([-117.3974]) / reward: 0.04\n",
      "실패1!\n",
      "False False\n",
      "2번째 시도 setPoint 37를 맞추기 위해\n",
      "2/10-0 - Action1 이후 x값과 예측 y값: (96, 346.95)에서 Up 하면 setPoint(y) 값은 tensor([-309.9452]) / reward: 0.02\n",
      "2/10-0 - Action2 이후 x값과 예측 y값: (95, 343.44)에서 Up 하면 setPoint(y) 값은 tensor([-306.4443]) / reward: 0.02\n",
      "2/10-1 - Action1 이후 x값과 예측 y값: (97, 350.45)에서 Up 하면 setPoint(y) 값은 tensor([-313.4460]) / reward: 0.02\n",
      "2/10-1 - Action2 이후 x값과 예측 y값: (95, 343.44)에서 Up 하면 setPoint(y) 값은 tensor([-306.4443]) / reward: 0.02\n",
      "2/10-2 - Action1 이후 x값과 예측 y값: (98, 353.95)에서 Up 하면 setPoint(y) 값은 tensor([-316.9469]) / reward: 0.02\n",
      "2/10-2 - Action2 이후 x값과 예측 y값: (95, 343.44)에서 Up 하면 setPoint(y) 값은 tensor([-306.4443]) / reward: 0.02\n",
      "2/10-3 - Action1 이후 x값과 예측 y값: (99, 357.45)에서 Up 하면 setPoint(y) 값은 tensor([-320.4478]) / reward: 0.02\n",
      "2/10-3 - Action2 이후 x값과 예측 y값: (95, 343.44)에서 Up 하면 setPoint(y) 값은 tensor([-306.4443]) / reward: 0.02\n",
      "2/10-4 - Action1 이후 x값과 예측 y값: (100, 360.95)에서 Up 하면 setPoint(y) 값은 tensor([-323.9486]) / reward: 0.02\n",
      "2/10-4 - Action2 이후 x값과 예측 y값: (95, 343.44)에서 Up 하면 setPoint(y) 값은 tensor([-306.4443]) / reward: 0.02\n",
      "2/10-5 - Action1 이후 x값과 예측 y값: (101, 360.95)에서 Up 하면 setPoint(y) 값은 None / reward: -100.0\n",
      "2/10-5 - Action2 이후 x값과 예측 y값: (95, 343.44)에서 Up 하면 setPoint(y) 값은 tensor([-306.4443]) / reward: 0.02\n",
      "실패1!\n",
      "False False\n",
      "3번째 시도 setPoint 37를 맞추기 위해\n",
      "3/10-0 - Action1 이후 x값과 예측 y값: (47, 175.4)에서 Up 하면 setPoint(y) 값은 tensor([-138.4026]) / reward: 0.04\n",
      "3/10-0 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-1 - Action1 이후 x값과 예측 y값: (48, 178.9)에서 Up 하면 setPoint(y) 값은 tensor([-141.9035]) / reward: 0.04\n",
      "3/10-1 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-2 - Action1 이후 x값과 예측 y값: (49, 182.4)에서 Up 하면 setPoint(y) 값은 tensor([-145.4044]) / reward: 0.03\n",
      "3/10-2 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-3 - Action1 이후 x값과 예측 y값: (50, 185.91)에서 Up 하면 setPoint(y) 값은 tensor([-148.9052]) / reward: 0.03\n",
      "3/10-3 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-4 - Action1 이후 x값과 예측 y값: (51, 189.41)에서 Up 하면 setPoint(y) 값은 tensor([-152.4061]) / reward: 0.03\n",
      "3/10-4 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-5 - Action1 이후 x값과 예측 y값: (52, 192.91)에서 Up 하면 setPoint(y) 값은 tensor([-155.9070]) / reward: 0.03\n",
      "3/10-5 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-6 - Action1 이후 x값과 예측 y값: (53, 196.41)에서 Up 하면 setPoint(y) 값은 tensor([-159.4078]) / reward: 0.03\n",
      "3/10-6 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-7 - Action1 이후 x값과 예측 y값: (54, 199.91)에서 Up 하면 setPoint(y) 값은 tensor([-162.9087]) / reward: 0.03\n",
      "3/10-7 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-8 - Action1 이후 x값과 예측 y값: (55, 203.41)에서 Up 하면 setPoint(y) 값은 tensor([-166.4096]) / reward: 0.03\n",
      "3/10-8 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-9 - Action1 이후 x값과 예측 y값: (56, 206.91)에서 Up 하면 setPoint(y) 값은 tensor([-169.9104]) / reward: 0.03\n",
      "3/10-9 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-10 - Action1 이후 x값과 예측 y값: (57, 210.41)에서 Up 하면 setPoint(y) 값은 tensor([-173.4113]) / reward: 0.03\n",
      "3/10-10 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-11 - Action1 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "3/10-11 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-12 - Action1 이후 x값과 예측 y값: (59, 217.41)에서 Up 하면 setPoint(y) 값은 tensor([-180.4130]) / reward: 0.03\n",
      "3/10-12 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-13 - Action1 이후 x값과 예측 y값: (60, 220.91)에서 Up 하면 setPoint(y) 값은 tensor([-183.9139]) / reward: 0.03\n",
      "3/10-13 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-14 - Action1 이후 x값과 예측 y값: (61, 224.41)에서 Up 하면 setPoint(y) 값은 tensor([-187.4148]) / reward: 0.03\n",
      "3/10-14 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-15 - Action1 이후 x값과 예측 y값: (62, 227.92)에서 Up 하면 setPoint(y) 값은 tensor([-190.9156]) / reward: 0.03\n",
      "3/10-15 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-16 - Action1 이후 x값과 예측 y값: (63, 231.42)에서 Up 하면 setPoint(y) 값은 tensor([-194.4165]) / reward: 0.03\n",
      "3/10-16 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-17 - Action1 이후 x값과 예측 y값: (64, 234.92)에서 Up 하면 setPoint(y) 값은 tensor([-197.9174]) / reward: 0.03\n",
      "3/10-17 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-18 - Action1 이후 x값과 예측 y값: (65, 238.42)에서 Up 하면 setPoint(y) 값은 tensor([-201.4182]) / reward: 0.02\n",
      "3/10-18 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-19 - Action1 이후 x값과 예측 y값: (66, 241.92)에서 Up 하면 setPoint(y) 값은 tensor([-204.9191]) / reward: 0.02\n",
      "3/10-19 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-20 - Action1 이후 x값과 예측 y값: (67, 245.42)에서 Up 하면 setPoint(y) 값은 tensor([-208.4200]) / reward: 0.02\n",
      "3/10-20 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-21 - Action1 이후 x값과 예측 y값: (68, 248.92)에서 Up 하면 setPoint(y) 값은 tensor([-211.9209]) / reward: 0.02\n",
      "3/10-21 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-22 - Action1 이후 x값과 예측 y값: (69, 252.42)에서 Up 하면 setPoint(y) 값은 tensor([-215.4217]) / reward: 0.02\n",
      "3/10-22 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-23 - Action1 이후 x값과 예측 y값: (70, 255.92)에서 Up 하면 setPoint(y) 값은 tensor([-218.9226]) / reward: 0.02\n",
      "3/10-23 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-24 - Action1 이후 x값과 예측 y값: (71, 259.42)에서 Up 하면 setPoint(y) 값은 tensor([-222.4234]) / reward: 0.02\n",
      "3/10-24 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-25 - Action1 이후 x값과 예측 y값: (72, 262.92)에서 Up 하면 setPoint(y) 값은 tensor([-225.9243]) / reward: 0.02\n",
      "3/10-25 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-26 - Action1 이후 x값과 예측 y값: (73, 266.43)에서 Up 하면 setPoint(y) 값은 tensor([-229.4252]) / reward: 0.02\n",
      "3/10-26 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-27 - Action1 이후 x값과 예측 y값: (74, 269.93)에서 Up 하면 setPoint(y) 값은 tensor([-232.9261]) / reward: 0.02\n",
      "3/10-27 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-28 - Action1 이후 x값과 예측 y값: (75, 273.43)에서 Up 하면 setPoint(y) 값은 tensor([-236.4269]) / reward: 0.02\n",
      "3/10-28 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-29 - Action1 이후 x값과 예측 y값: (76, 276.93)에서 Up 하면 setPoint(y) 값은 tensor([-239.9278]) / reward: 0.02\n",
      "3/10-29 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-30 - Action1 이후 x값과 예측 y값: (77, 280.43)에서 Up 하면 setPoint(y) 값은 tensor([-243.4287]) / reward: 0.02\n",
      "3/10-30 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-31 - Action1 이후 x값과 예측 y값: (78, 283.93)에서 Up 하면 setPoint(y) 값은 tensor([-246.9295]) / reward: 0.02\n",
      "3/10-31 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-32 - Action1 이후 x값과 예측 y값: (79, 287.43)에서 Up 하면 setPoint(y) 값은 tensor([-250.4304]) / reward: 0.02\n",
      "3/10-32 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-33 - Action1 이후 x값과 예측 y값: (80, 290.93)에서 Up 하면 setPoint(y) 값은 tensor([-253.9313]) / reward: 0.02\n",
      "3/10-33 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-34 - Action1 이후 x값과 예측 y값: (81, 294.43)에서 Up 하면 setPoint(y) 값은 tensor([-257.4321]) / reward: 0.02\n",
      "3/10-34 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-35 - Action1 이후 x값과 예측 y값: (82, 297.93)에서 Up 하면 setPoint(y) 값은 tensor([-260.9330]) / reward: 0.02\n",
      "3/10-35 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-36 - Action1 이후 x값과 예측 y값: (83, 301.43)에서 Up 하면 setPoint(y) 값은 tensor([-264.4339]) / reward: 0.02\n",
      "3/10-36 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-37 - Action1 이후 x값과 예측 y값: (84, 304.93)에서 Up 하면 setPoint(y) 값은 tensor([-267.9347]) / reward: 0.02\n",
      "3/10-37 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-38 - Action1 이후 x값과 예측 y값: (85, 308.44)에서 Up 하면 setPoint(y) 값은 tensor([-271.4356]) / reward: 0.02\n",
      "3/10-38 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-39 - Action1 이후 x값과 예측 y값: (86, 311.94)에서 Up 하면 setPoint(y) 값은 tensor([-274.9365]) / reward: 0.02\n",
      "3/10-39 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-40 - Action1 이후 x값과 예측 y값: (87, 315.44)에서 Up 하면 setPoint(y) 값은 tensor([-278.4373]) / reward: 0.02\n",
      "3/10-40 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-41 - Action1 이후 x값과 예측 y값: (88, 318.94)에서 Up 하면 setPoint(y) 값은 tensor([-281.9382]) / reward: 0.02\n",
      "3/10-41 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-42 - Action1 이후 x값과 예측 y값: (89, 322.44)에서 Up 하면 setPoint(y) 값은 tensor([-285.4391]) / reward: 0.02\n",
      "3/10-42 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-43 - Action1 이후 x값과 예측 y값: (90, 325.94)에서 Up 하면 setPoint(y) 값은 tensor([-288.9399]) / reward: 0.02\n",
      "3/10-43 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-44 - Action1 이후 x값과 예측 y값: (91, 329.44)에서 Up 하면 setPoint(y) 값은 tensor([-292.4408]) / reward: 0.02\n",
      "3/10-44 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-45 - Action1 이후 x값과 예측 y값: (92, 332.94)에서 Up 하면 setPoint(y) 값은 tensor([-295.9417]) / reward: 0.02\n",
      "3/10-45 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-46 - Action1 이후 x값과 예측 y값: (93, 336.44)에서 Up 하면 setPoint(y) 값은 tensor([-299.4425]) / reward: 0.02\n",
      "3/10-46 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-47 - Action1 이후 x값과 예측 y값: (94, 339.94)에서 Up 하면 setPoint(y) 값은 tensor([-302.9434]) / reward: 0.02\n",
      "3/10-47 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-48 - Action1 이후 x값과 예측 y값: (95, 343.44)에서 Up 하면 setPoint(y) 값은 tensor([-306.4443]) / reward: 0.02\n",
      "3/10-48 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-49 - Action1 이후 x값과 예측 y값: (96, 346.95)에서 Up 하면 setPoint(y) 값은 tensor([-309.9452]) / reward: 0.02\n",
      "3/10-49 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-50 - Action1 이후 x값과 예측 y값: (97, 350.45)에서 Up 하면 setPoint(y) 값은 tensor([-313.4460]) / reward: 0.02\n",
      "3/10-50 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-51 - Action1 이후 x값과 예측 y값: (98, 353.95)에서 Up 하면 setPoint(y) 값은 tensor([-316.9469]) / reward: 0.02\n",
      "3/10-51 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-52 - Action1 이후 x값과 예측 y값: (99, 357.45)에서 Up 하면 setPoint(y) 값은 tensor([-320.4478]) / reward: 0.02\n",
      "3/10-52 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-53 - Action1 이후 x값과 예측 y값: (100, 360.95)에서 Up 하면 setPoint(y) 값은 tensor([-323.9486]) / reward: 0.02\n",
      "3/10-53 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "3/10-54 - Action1 이후 x값과 예측 y값: (101, 360.95)에서 Up 하면 setPoint(y) 값은 None / reward: -100.0\n",
      "3/10-54 - Action2 이후 x값과 예측 y값: (46, 171.9)에서 Up 하면 setPoint(y) 값은 tensor([-134.9017]) / reward: 0.04\n",
      "실패1!\n",
      "False False\n",
      "4번째 시도 setPoint 37를 맞추기 위해\n",
      "4/10-0 - Action1 이후 x값과 예측 y값: (59, 217.41)에서 Up 하면 setPoint(y) 값은 tensor([-180.4130]) / reward: 0.03\n",
      "4/10-0 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-1 - Action1 이후 x값과 예측 y값: (60, 220.91)에서 Up 하면 setPoint(y) 값은 tensor([-183.9139]) / reward: 0.03\n",
      "4/10-1 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-2 - Action1 이후 x값과 예측 y값: (61, 224.41)에서 Up 하면 setPoint(y) 값은 tensor([-187.4148]) / reward: 0.03\n",
      "4/10-2 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-3 - Action1 이후 x값과 예측 y값: (62, 227.92)에서 Up 하면 setPoint(y) 값은 tensor([-190.9156]) / reward: 0.03\n",
      "4/10-3 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-4 - Action1 이후 x값과 예측 y값: (63, 231.42)에서 Up 하면 setPoint(y) 값은 tensor([-194.4165]) / reward: 0.03\n",
      "4/10-4 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-5 - Action1 이후 x값과 예측 y값: (64, 234.92)에서 Up 하면 setPoint(y) 값은 tensor([-197.9174]) / reward: 0.03\n",
      "4/10-5 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-6 - Action1 이후 x값과 예측 y값: (65, 238.42)에서 Up 하면 setPoint(y) 값은 tensor([-201.4182]) / reward: 0.02\n",
      "4/10-6 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-7 - Action1 이후 x값과 예측 y값: (66, 241.92)에서 Up 하면 setPoint(y) 값은 tensor([-204.9191]) / reward: 0.02\n",
      "4/10-7 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-8 - Action1 이후 x값과 예측 y값: (67, 245.42)에서 Up 하면 setPoint(y) 값은 tensor([-208.4200]) / reward: 0.02\n",
      "4/10-8 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-9 - Action1 이후 x값과 예측 y값: (66, 241.92)에서 Down 하면 setPoint(y) 값은 tensor([-204.9191]) / reward: 0.02\n",
      "4/10-9 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-10 - Action1 이후 x값과 예측 y값: (67, 245.42)에서 Up 하면 setPoint(y) 값은 tensor([-208.4200]) / reward: 0.02\n",
      "4/10-10 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-11 - Action1 이후 x값과 예측 y값: (68, 248.92)에서 Up 하면 setPoint(y) 값은 tensor([-211.9209]) / reward: 0.02\n",
      "4/10-11 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-12 - Action1 이후 x값과 예측 y값: (69, 252.42)에서 Up 하면 setPoint(y) 값은 tensor([-215.4217]) / reward: 0.02\n",
      "4/10-12 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-13 - Action1 이후 x값과 예측 y값: (70, 255.92)에서 Up 하면 setPoint(y) 값은 tensor([-218.9226]) / reward: 0.02\n",
      "4/10-13 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-14 - Action1 이후 x값과 예측 y값: (71, 259.42)에서 Up 하면 setPoint(y) 값은 tensor([-222.4234]) / reward: 0.02\n",
      "4/10-14 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-15 - Action1 이후 x값과 예측 y값: (72, 262.92)에서 Up 하면 setPoint(y) 값은 tensor([-225.9243]) / reward: 0.02\n",
      "4/10-15 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-16 - Action1 이후 x값과 예측 y값: (73, 266.43)에서 Up 하면 setPoint(y) 값은 tensor([-229.4252]) / reward: 0.02\n",
      "4/10-16 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-17 - Action1 이후 x값과 예측 y값: (74, 269.93)에서 Up 하면 setPoint(y) 값은 tensor([-232.9261]) / reward: 0.02\n",
      "4/10-17 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-18 - Action1 이후 x값과 예측 y값: (75, 273.43)에서 Up 하면 setPoint(y) 값은 tensor([-236.4269]) / reward: 0.02\n",
      "4/10-18 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-19 - Action1 이후 x값과 예측 y값: (76, 276.93)에서 Up 하면 setPoint(y) 값은 tensor([-239.9278]) / reward: 0.02\n",
      "4/10-19 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-20 - Action1 이후 x값과 예측 y값: (77, 280.43)에서 Up 하면 setPoint(y) 값은 tensor([-243.4287]) / reward: 0.02\n",
      "4/10-20 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-21 - Action1 이후 x값과 예측 y값: (78, 283.93)에서 Up 하면 setPoint(y) 값은 tensor([-246.9295]) / reward: 0.02\n",
      "4/10-21 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-22 - Action1 이후 x값과 예측 y값: (79, 287.43)에서 Up 하면 setPoint(y) 값은 tensor([-250.4304]) / reward: 0.02\n",
      "4/10-22 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-23 - Action1 이후 x값과 예측 y값: (80, 290.93)에서 Up 하면 setPoint(y) 값은 tensor([-253.9313]) / reward: 0.02\n",
      "4/10-23 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-24 - Action1 이후 x값과 예측 y값: (81, 294.43)에서 Up 하면 setPoint(y) 값은 tensor([-257.4321]) / reward: 0.02\n",
      "4/10-24 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-25 - Action1 이후 x값과 예측 y값: (82, 297.93)에서 Up 하면 setPoint(y) 값은 tensor([-260.9330]) / reward: 0.02\n",
      "4/10-25 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-26 - Action1 이후 x값과 예측 y값: (83, 301.43)에서 Up 하면 setPoint(y) 값은 tensor([-264.4339]) / reward: 0.02\n",
      "4/10-26 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-27 - Action1 이후 x값과 예측 y값: (84, 304.93)에서 Up 하면 setPoint(y) 값은 tensor([-267.9347]) / reward: 0.02\n",
      "4/10-27 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-28 - Action1 이후 x값과 예측 y값: (85, 308.44)에서 Up 하면 setPoint(y) 값은 tensor([-271.4356]) / reward: 0.02\n",
      "4/10-28 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-29 - Action1 이후 x값과 예측 y값: (86, 311.94)에서 Up 하면 setPoint(y) 값은 tensor([-274.9365]) / reward: 0.02\n",
      "4/10-29 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-30 - Action1 이후 x값과 예측 y값: (87, 315.44)에서 Up 하면 setPoint(y) 값은 tensor([-278.4373]) / reward: 0.02\n",
      "4/10-30 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-31 - Action1 이후 x값과 예측 y값: (88, 318.94)에서 Up 하면 setPoint(y) 값은 tensor([-281.9382]) / reward: 0.02\n",
      "4/10-31 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-32 - Action1 이후 x값과 예측 y값: (89, 322.44)에서 Up 하면 setPoint(y) 값은 tensor([-285.4391]) / reward: 0.02\n",
      "4/10-32 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-33 - Action1 이후 x값과 예측 y값: (90, 325.94)에서 Up 하면 setPoint(y) 값은 tensor([-288.9399]) / reward: 0.02\n",
      "4/10-33 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-34 - Action1 이후 x값과 예측 y값: (91, 329.44)에서 Up 하면 setPoint(y) 값은 tensor([-292.4408]) / reward: 0.02\n",
      "4/10-34 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-35 - Action1 이후 x값과 예측 y값: (92, 332.94)에서 Up 하면 setPoint(y) 값은 tensor([-295.9417]) / reward: 0.02\n",
      "4/10-35 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-36 - Action1 이후 x값과 예측 y값: (93, 336.44)에서 Up 하면 setPoint(y) 값은 tensor([-299.4425]) / reward: 0.02\n",
      "4/10-36 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-37 - Action1 이후 x값과 예측 y값: (94, 339.94)에서 Up 하면 setPoint(y) 값은 tensor([-302.9434]) / reward: 0.02\n",
      "4/10-37 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-38 - Action1 이후 x값과 예측 y값: (95, 343.44)에서 Up 하면 setPoint(y) 값은 tensor([-306.4443]) / reward: 0.02\n",
      "4/10-38 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-39 - Action1 이후 x값과 예측 y값: (96, 346.95)에서 Up 하면 setPoint(y) 값은 tensor([-309.9452]) / reward: 0.02\n",
      "4/10-39 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-40 - Action1 이후 x값과 예측 y값: (97, 350.45)에서 Up 하면 setPoint(y) 값은 tensor([-313.4460]) / reward: 0.02\n",
      "4/10-40 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-41 - Action1 이후 x값과 예측 y값: (98, 353.95)에서 Up 하면 setPoint(y) 값은 tensor([-316.9469]) / reward: 0.02\n",
      "4/10-41 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-42 - Action1 이후 x값과 예측 y값: (99, 357.45)에서 Up 하면 setPoint(y) 값은 tensor([-320.4478]) / reward: 0.02\n",
      "4/10-42 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-43 - Action1 이후 x값과 예측 y값: (100, 360.95)에서 Up 하면 setPoint(y) 값은 tensor([-323.9486]) / reward: 0.02\n",
      "4/10-43 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "4/10-44 - Action1 이후 x값과 예측 y값: (101, 360.95)에서 Up 하면 setPoint(y) 값은 None / reward: -100.0\n",
      "4/10-44 - Action2 이후 x값과 예측 y값: (58, 213.91)에서 Up 하면 setPoint(y) 값은 tensor([-176.9122]) / reward: 0.03\n",
      "실패1!\n",
      "False False\n",
      "5번째 시도 setPoint 37를 맞추기 위해\n",
      "5/10-0 - Action1 이후 x값과 예측 y값: (100, 360.95)에서 Up 하면 setPoint(y) 값은 tensor([-323.9486]) / reward: 0.02\n",
      "5/10-0 - Action2 이후 x값과 예측 y값: (99, 357.45)에서 Up 하면 setPoint(y) 값은 tensor([-320.4478]) / reward: 0.02\n",
      "5/10-1 - Action1 이후 x값과 예측 y값: (101, 360.95)에서 Up 하면 setPoint(y) 값은 None / reward: -100.0\n",
      "5/10-1 - Action2 이후 x값과 예측 y값: (99, 357.45)에서 Up 하면 setPoint(y) 값은 tensor([-320.4478]) / reward: 0.02\n",
      "실패1!\n",
      "False False\n",
      "6번째 시도 setPoint 37를 맞추기 위해\n",
      "6/10-0 - Action1 이후 x값과 예측 y값: (2, 17.86)에서 Down 하면 setPoint(y) 값은 tensor([19.1364]) / reward: 0.26\n",
      "6/10-0 - Action2 이후 x값과 예측 y값: (4, 24.87)에서 Up 하면 setPoint(y) 값은 tensor([12.1347]) / reward: 0.41\n",
      "6/10-1 - Action1 이후 x값과 예측 y값: (1, 14.36)에서 Down 하면 setPoint(y) 값은 tensor([22.6373]) / reward: 0.22\n",
      "6/10-1 - Action2 이후 x값과 예측 y값: (5, 28.37)에서 Up 하면 setPoint(y) 값은 tensor([8.6338]) / reward: 0.58\n",
      "6/10-2 - Action1 이후 x값과 예측 y값: (0, 10.86)에서 Down 하면 setPoint(y) 값은 tensor([26.1382]) / reward: 0.19\n",
      "6/10-2 - Action2 이후 x값과 예측 y값: (6, 31.87)에서 Up 하면 setPoint(y) 값은 tensor([5.1330]) / reward: 0.97\n",
      "6/10-3 - Action1 이후 x값과 예측 y값: (-1, 7.36)에서 Down 하면 setPoint(y) 값은 tensor([29.6391]) / reward: 0.17\n",
      "6/10-3 - Action2 이후 x값과 예측 y값: (7, 35.37)에서 Up 하면 setPoint(y) 값은 tensor([1.6321]) / reward: 3.06\n",
      "성공2! 입력:7, 출력:35.37, 목표:37\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 이 state에서 setPoint가 되기 위해서 어떤 action을 취하라 \n",
    "num_episodes = 10 \n",
    "stop_sign=False\n",
    "for ep in range(num_episodes):\n",
    "\n",
    "    print(f\"{ep}번째 시도 setPoint {setPoint}를 맞추기 위해\")\n",
    "    z = random.randrange(-10, 100)\n",
    "    env1 = Environment(z=z, setPoint=setPoint)\n",
    "    env2 = Environment(z=z, setPoint=setPoint)\n",
    "\n",
    "    yPred = model.predict(np.array([z]).reshape(-1,1)).item()\n",
    "    state1 = torch.tensor([setPoint - yPred])    # 지금 y 값\n",
    "    state2 = torch.tensor([setPoint - yPred])    # 지금 y 값\n",
    "\n",
    "    initial_action = select_action(torch.tensor([state1]).float())\n",
    "\n",
    "    msg_list1 = []\n",
    "    msg_list2 = []\n",
    "\n",
    "    process1 = True \n",
    "    process2 = True \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for t in count():\n",
    "            if process1:\n",
    "                action1 = select_action(torch.tensor([state1]).float())\n",
    "                next_state1, reward1, done1 = env1.step(action1.item())\n",
    "                reward1 = torch.tensor([reward1], device=device)\n",
    "                msg1 = f\"{ep}/{num_episodes}-{t} - Action1 이후 x값과 예측 y값: {env1.render()}에서 {Action(action1)} 하면 setPoint(y) 값은 {next_state1} / reward: {round(reward1.item(),2)}\"\n",
    "                print(msg1)\n",
    "                msg_list1.append(msg1)\n",
    "                # 다음 상태로 이동 \n",
    "                state1 = next_state1 \n",
    "            \n",
    "            if process2:\n",
    "                action2 = initial_action+1 % 2\n",
    "                next_state2, reward2, done2 = env2.step(action2.item())\n",
    "                reward2 = torch.tensor([reward2], device=device)\n",
    "                msg2 = f\"{ep}/{num_episodes}-{t} - Action2 이후 x값과 예측 y값: {env2.render()}에서 {Action(action2)} 하면 setPoint(y) 값은 {next_state2} / reward: {round(reward2.item(),2)}\"\n",
    "                print(msg2)\n",
    "                msg_list2.append(msg2)\n",
    "                # 다음 상태로 이동 \n",
    "                state2 = next_state2\n",
    "            \n",
    "            \n",
    "            if done1:\n",
    "                cur_x,pred = env1.render()\n",
    "                if -10 <= cur_x <= 100:\n",
    "                    print(f\"성공1! 입력:{cur_x}, 출력:{pred}, 목표:{setPoint}\")\n",
    "                    stop_sign = True \n",
    "                    print()\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"실패1!\")\n",
    "                    done1 = False\n",
    "                    print(done1,done2)\n",
    "                    if done1 == False and done2 == False:\n",
    "                        break\n",
    "            \n",
    "            if done2:\n",
    "                cur_x,pred = env2.render()\n",
    "                if -10 <= cur_x <= 100:\n",
    "                    print(f\"성공2! 입력:{cur_x}, 출력:{pred}, 목표:{setPoint}\")\n",
    "                    stop_sign = True \n",
    "                    print()\n",
    "                    break\n",
    "                else:\n",
    "                    print(\"실패2!\")\n",
    "                    done2 = False\n",
    "                    print(done1,done2)\n",
    "                    if done1 == False and done2 == False:\n",
    "                        break\n",
    "    if stop_sign:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x 입력: 7, 출력: 35.367892081236434, 목표 y 값: 37\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(np.array([cur_x]).reshape(-1,1)).item()\n",
    "\n",
    "print(f'x 입력: {cur_x}, 출력: {pred}, 목표 y 값: {setPoint}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='x', ylabel='y'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data.plot(x='x', y='y', kind='scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
